Implementations of Data Analysis Tools Into the Biomedical Modular System

Abstract—Systems that utilize biological data typically necessitate a module with the capacity to analyze biomedical data. This article investigates the problem at the specified location and outlines the potential solutions at our disposal. We'll examine the deployed technology that can be bundled as a container for a microservice architecture and provides data analysis capabilities. Each module must contain fundamental features and capabilities, such as fuzzy machine learning and data mining methods. This component must be able to generate an accurate report based on the results of its analysis. The primary benefit of such a system is its modularity, enabling quick adaptations and expansion for various data types and simpler adaptation of the modules to newer technologies.

Keywords—dotNet; IKVMdotNET; Weka; D3.js; Biobank.

INTRODUCTION

Currently, humanity strives to store as many distinct types of data as feasible. This information is gathered using various scientific methods. In the discipline of biomedicine, the same holds true. Both digital and analog representations of biomedical data are possible. Data are stored in anticipation of future processing or retrospective research. Digital data are stored in expansive databases subdivided by a target area. Analog biomedical data such as cell, tissue, and blood samples are stored at low temperatures in specialized facilities.

Disease and its treatment have always been inextricably tied to observation and data analysis. Whether we examine ancient Greek literature detailing diseases and their treatment or modern physicians' use of advanced laboratory and radiological testing, it is evident that data gathering and interpretation play an important part in healthcare delivery. Biomedical data can be collected either with or without patients' consent. Obtaining data without patient consent is associated with more significant restrictions and issues regarding patient privacy. The types of data we may retain include basic information such as the date of birth and sex of the patient. These are also routine medical details such as height, weight, blood pressure, cholesterol levels, and the medication the patient takes. Protein, lipid, and metabolite values are also among the biomedical data that must be stored in specific cases.

Databanks that store biomedical data must meet all necessary data protection and security standards. Data are only made available to researchers and institutions authorized to handle and analyze them. The results of analyses applied to biomedical data can help humanity better understand the biological processes in the human body. Examples of these processes include the study of the spread of diseases and viruses. In addition, the results can help develop new antibodies and vaccines that protect people from dangerous conditions. Biomedical scientists generate, collect, and keep more research data than ever. They are doing so at a time that demands ever greater levels of openness and data-sharing opportunities while increasing emphasis on protecting the privacy of individuals whose data originated. Therefore, it is essential to store this data in a way that makes it easily accessible. But nowadays, it is not realistic to collect data with the expectation that it will be kept forever without cost.

The paper aims to expand and improve the functionality of the provided biomedical system with additional tools for the analysis of several types of biomedical data. The efficiency of the data analysis tools will be achieved utilizing the microservice approach to implement different modules for different data types.

Similar biomedical systems

Several technologies now exist which allow the storage and subsequent analysis of biological data. The authors of the publication discuss a translational medicine platform that permits integration with UK biobank data and analysis using the capability provided. In another publication, the authors present the BiBBoX architecture for use in nations participating in the B3Africa project. This framework's design is primarily focused on boosting knowledge sharing and information exchange in these nations for illness treatment. Another example of a system design is the GIMC architecture adopted by the authors for use in Valencia's scientific medical community. All of these systems are designed with a certain community's needs and a limited set of applications in mind. As a result, they cannot be used generically. The designs of these systems do have one thing in common: they all make use of cutting-edge techniques such as microservice architecture and the capacity to share and modularize.

BIOBANK

Biomedical data is usually stored in biobanks. Although "biobank" first appeared in professional publications in 1996, no clear and universally accepted definition remains. This definition was later updated to describe biobanks as structured resources suitable for genetic research, comprising human biological material and data derived from the analysis of such material, together with extensive associated information. A biobank can also be defined as a set of samples of biological material such as blood. Its size is variable. Smaller biobanks contain hundreds of examples, more significant ones thousands. The type of data and associated samples stored depends on the purpose of the biobank. Often they are focused only on a particular kind of disease, such as cancer. The goal may be the disease and the region, for example. In this case, data on the inhabitants of a specific region is stored. In addition, universities, research centers, hospitals, and disease-specific organizations use biobanks to store data. Biobanks can be categorized based on the types of samples they hold.

Tissue biobanks contain tissues obtained from autopsies or surgical procedures, which are then stored for histological analysis. The temperature at which the tissues should be stored is -80°C. This temperature should be achieved using liquid nitrogen to prevent contamination by floating tissue fragments.

Blood biobanks contain blood samples, specifically blood serum, blood cells, or blood plasma. All blood components mentioned above must be stored in closed tubes mixed with the necessary preservatives or appropriate additives for subsequent analysis. Blood serum can be used for biochemical and blood plasma for DNA analysis. Samples can be stored for the short or long term. For short-term storage, a temperature of -20 °C is reached, and long-term storage up to -80 °C.

Cell biobanks store harvested cellular material essential for biomedical research. Some essential cell biobanks include the Korean Cell Line Bank and the American Type Culture Collection, which significantly provide access to cellular material to researchers who need it for their studies.

Organoid biobanks store miniature three-dimensional simplified organ replicas called organoids. These replicas are kept in test tubes and are made from stem cells. Their function and composition simulate the operation and design of the corresponding absolute authority. These biobanks play an essential role in biomedical research and medicine specializing in tissue regeneration. Currently, scientists can create organoids that mimic the actions of the kidneys, stomach, lungs, or brain.

Digital biobanks store data exclusively in digitized form. They provide a means by which data generated from research on biological material can be linked with data produced from clinical studies or relevant research institutions. The gradual improvement in digital biobanks has led to the need for specific standards. This standardization has been introduced in collecting, storing, and processing biological material samples. In digital biobanks, data are stored with a vision for future research and conducting retrospective studies. Among the leading advantages include the possibility of creating a network of digital biobanks, which enables easy collaboration between different institutions involved in research in the field. This reduces the costs that would have to be spent on the collection and processing of biological material.

Population biobanks can consist of thousands of samples of biological material of different species. Samples are obtained from specific populations that share a common pathology. Examples of pathologies include diabetes or neuromuscular disease. Representatives may only be accepted from individual patients with their informed consent. In the informed consent, the patient is informed of the rationale for collecting biological material, the possible spread of and sharing, and how to use it in future biomedical research. The patient has the right to withdraw consent to sample processing at any time. He shall also have the right to be informed of the studies' results on the samples submitted.

STATE OF THE IMPLEMENTED SYSTEM

The original application provided the essential functions of the information system. It has an intuitive user interface. This application was designed and implemented in a system of biomedical data at the Faculty of Management and Informatics of the University of Žilina in Žilina.

The first step to using the app is user registration itself or logging in. There are two ways to log in to the application. The application implements the option of logging in with a Microsoft account or registering to the application separately. The user is redirected to the company's login page when logging in with a Microsoft account. The home page of the application shows the essential functional elements. A user information icon is displayed at the top right. Clicking on this icon displays basic information about the user logged into the application. There is also an option to redirect to the user profile and an icon for logging out. On the left side of the home screen is a panel used to navigate the application. With the help of the board, it is possible to navigate despite all the application functions. The individual implemented procedures are divided into respective categories. The center of the home screen is filled with tabs that allow the user to navigate through the basic functions of the application quickly. Each tab has a description and the corresponding icon for a more intuitive orientation in the application.

When the user clicks Profile, the main Profile details are displayed. These details include the roles assigned to the user and their first and last name, which the user can change if they wish. If the logged-in user has the Administrator role, the Administration item is displayed in the sidebar for that user. In Administration, the admin can change basic application settings such as sending email notifications or the ability to log in with a Microsoft account. In addition to the settings for the application itself, the administrator can also manage the accounts of all registered users. For example, he has the right to change a user's first name, last name, password, and email or to assign or remove a role. Administrator rights also include removing the user account altogether or creating a new account, which the user will be informed about by email with a request for activation.

The Project Manager role can manage the created projects in the navigation bar. The manager has the right to develop projects, delete tasks, modify project data, and manage users assigned to specific projects.

The database in the navigation bar groups all implemented functionality for data processing. The uploaded data must be of CSV type. For the data upload process to be successful, the user with the Data Manager role must select the correct value separator by which the file values are separated. In addition, it is necessary to specify the name under which the data will later be represented, in case it is also possible to specify a description of the uploaded data for easier identification. If the uploaded data contains a header in the first line, it can be loaded by selecting the Load Header option.

After uploading the data, there is an option to list all the data stored in the database along with its description. The name and description can be changed, or the data can be deleted from the database. It is possible to create views over the data by selecting columns. It is also necessary to enter a description for each view. All data views can be managed in the Data View option. Click on the table name to display the views created above this table, which can then be renamed or deleted. Views represent a subset of data the user makes over which the user can perform analytical functions.

The operator role allows the user to access projects the project manager has granted access to. If data in the database or relevant data views are associated with the project, the appropriate analyses can be performed within the project. Once a project is selected, the option to open the scheme is available, giving the user access to work with the project. Closing the project is done in the same way.

SYSTEM ARCHITECTURE

The architecture of the implemented application is divided into two basic units, which include the client and server parts. The individual components are decoupled from each other. The communication between them is done using forwarded messages. The server part consists of modules that provide the application's basic functionality, such as sending emails, user authentication, data manipulation, or data analysis. A graphical representation of the architecture and the connection of the modules with the database and the client part can be seen.

Weka's Java byte code (weka.jar file) must be converted to a DLL file for the platform.NET. IKVM.NET runs Weka and translates using its full features. Microsoft.NET JVM IKVM.NET. A JVM, or Java Virtual Machine, runs Java and Groovy/Kotlin code. Winward Studios revived the open-source product three years after its 2015 discontinuation. 2022 saw.NET Core support. IKVM.NET converts Java code for usage in.NET projects.

ANALYSIS TOOLS

Opening the data project allows for implemented analytics. Projects features an Analysis tab. Project opening unblocks the tab. This tab displays data basics. Analysis cannot continue without selected data. The list of studies is provided below the data table. Incorrect data appears onscreen. Included among the errors are data with missing data and duplicate data identifiers (headers). The user can alter or keep the data. After analysis, tabs display the results. The tabs show plotted graphs, the analyses' written output, and a PDF download option.

Rendering Data

Data visualization uses D3.js and Canvg. Hierarchical clustering's dendrogram is rendered by D3.js. Analysis produces a Newick dendrogram. This subchapter describes, designs, and illustrates the Newick format. Reformat it to JSON to render it rapidly with D3.js. The bio-js-Newick library reformats. D3.js converts the JSON into SVG elements, which the Canvg library renders as a PNG image. This image can be downloaded by clicking the bio-js-Newick library's Newick-to-JSON button. The server reformats the J48 and Random Tree classification's DOT file into SVG elements, which the web client renders using the Canvg package.

The server part of the module

On the server side of the application and on the client side, improvements and new functionality in the data upload area were implemented. The new functionality includes implementing a custom library for working with XLSX files. In the Analytical module, further analyses were implemented using the Weka library. To perform the selected data analyses, the necessary validations have been programmed to verify the correctness of the data or the duplication of the headers.

Analysis of Data

To run the analysis using the Weka library, the data had to be prepared and saved as an ARFF file. The web client sends the server table identifiers and analysis type. All specified tables' data is read from the database into a DataTable class array. After data retrieval, column values representing specific patients are checked first. After checking all selected tables, the client receives a list of all erroneous columns and the table name. The user writes an error message and defective columns. If the user ignores erroneous columns, the analysis will not use their data. This info cannot be edited in the program. After column data validation, validate duplicate headers against all specified tables. Compare tables, not cells. The client program receives a list of matching header names and the table name. The user can manually rename headers or leave them alone. The server receives the new header names and the user's decision to keep them. The Weka library was utilized for all analyses. The generated ARFF file serves as the data source for each function.

K-Means clustering

The first analysis implemented is K-Means clustering. The number of clusters is defined as the number of unique class names (samples), with the minimum number of clusters set to 3. To detect the assignment of each class to clusters, it was necessary to explicitly put the writing of this information into the resulting text output. The text output can be retrieved after the clustering is complete using a function implemented in the Weka library. Since K-Means clustering does not have the option of visualization of the output, only the output in text format is sent to the client part, which contains information about the assignment of classes to particular clusters, the number of iterations of clustering, the starting classes, the resulting centroids, and the number of elements assigned to specific groups.

Hierarchical clustering

The second analysis implemented is Hierarchical Clustering, where the correct setting of the necessary parameters was required. We chose the nearest neighbor method, Complete Linkage, as a clustering method. The distances between objects are calculated using Euclidean distance. The data was assigned an index at which the classes (name) of the sample were located. To plot the dendrogram, it is necessary to enable Newick format output explicitly. The text output also contains information about the assignment of the classes to clusters and their counts in each cluster.

J48 classification

Weka library class J48 generates C4.5 decision trees. The minimal number of objects (sons) per leaf, decimal places in the tree, and object data storage have been specifically defined. We set each leaf's minimum sons to 1. Data precision necessitated 4 decimal places. The data has to be added to the index with sample classes (names). Lastly, the graph() function is used to acquire the DOT format required for plotting.

RandomTree classification

RandomTree is a class in the Weka library that is used to construct decision trees. Each vertex gets K random properties during creation. K, the maximum tree depth, the number of decimal places, and the number of instances to be processed if batch prediction is done must be defined. K, which indicates random qualities, was modified from 0 to 1. Next, we set the tree's depth to zero, which indicates an unbounded value. Due to data accuracy, the J48 categorization set the decimal places to 4. Due to the precision of the resulting tree, the number of processing iterations is set to 1000. The graph() function statement is used to obtain the DOT format, same as it is in the J48 categorization. In addition, we must assign an index to the data where the sample classes (names) reside.

Rendering data

The web element of the system required capabilities to reformat Weka data into the correct format. In the case of hierarchical clustering, it was critical to specify the names of the terminal peaks in the Newick output. The Newick format initially wrote simply the vertex indices in the vertex name places. These indices represent the class's data location and could be confused for the category name. For J48 and Random Tree classification, the output needed for plotting was data in DOT format. We converted the DOT data into SVG elements to make the graphical output easier to render on the web, using Graphviz.Netwrapper library.

CONCLUSION

Throughout the implementation, the system was regularly tested, and any faults discovered were quickly corrected. Following the completion of the performance, the new but corrected application functionality was tested. In the client part, the functionality was carefully tested for all potential cases. For example, while uploading data, it was essential to test the proper operation of the file type validation, which resulted in several capabilities being blocked. It was also necessary to verify the appearance of the tooltip in the field where the user inputs the file. The inclusion of a loading window, which is only required when the server component is processing the request, was also tried. After finishing the analysis, the user is presented with the findings, which are separated into three tabs. The Graph tab displays a graphical depiction of the relevant tree, the Text Output tab displays the analysis's text output, and the Report tab allows the user to download the report in PDF or TXT format. graphical representation The hierarchical clustering findings are separated into many dendrograms, which are displayed in the tab below. The decision tree in the case of J48 and Random Tree classification may be viewed using the scrolling at the bottom of the page. All graphical results are available for download by the user.

This article examined a number of technologies that provide machine learning, prediction, or data categorization capabilities. Because they give several alternatives, these libraries can be employed in future improvements. In addition, an analytics module was added to the program, which performed sophisticated analysis using the Weka library. The analysis module allows for the future introduction of other analyses. Because the Weka library employs a specific ARFF file type as an input data source, the module has the ability to generate these file types. In addition to creating ARFF files, the system module supports the creation of CSV files. The key advantage of the analysis module is its modularity, since any other module for a different type of data, analysis, or technological implementation may be developed and interpolated into the existing system. The same is true when restructuring or improving one of the modules. The remaining modules can be used while one module is being implemented.

The tool also has the ability to generate and download reports of completed analysis. The announcements include a summary of the research, the data used to perform the study, and the investigation's written output. In the case of hierarchical clustering, the PDF report includes the appropriate plotted dendrograms. In addition to PDF material, the option to download reports in TXT format has been added. The key advantage of the analysis module is its modularity, since any other module for a different type of data, analysis, or technological implementation may be developed and interpolated into the existing system. The same is true when restructuring or improving one of the modules. The remaining modules can be used while one module is being implemented. Possible application extensions include the implementation of additional analyses using model reliability analysis, the ability to store study results on a server for future review, or the ability to send the analysis of the results to the user's email.