An automatic model management system and its implementation for AIOps on microservice platforms

Abstract:
With the gradual expansion of microservice architecture-based applications, the complexity of system operation and maintenance is also growing significantly. With the advent of AIOps, it is now possible to automatically detect the state of the system, allocate resources, warn, and detect anomalies using machine learning models. Given the dynamic nature of online workloads, the running state of a microservice system in production is constantly in flux. Therefore, it is necessary to continuously train, encapsulate, and deploy models based on the current system status for the AIOps model to dynamically adapt to the system environment. This paper proposes a model update and management pipeline framework for AIOps models in microservices systems to accomplish the aforementioned objectives and simplify the process. A prototype system based on Kubernetes and Gitlab is designed to provide preliminary framework implementation and validation. The system consists of three components: model training, model packaging, and model deploying. Parallelization and parameter search are incorporated into the model training procedure to facilitate rapid training of multiple models and automated model hyperparameter tuning. We automate the packaging and deployment process using technology for continuous integration. Experiments are conducted to validate the prototype system, and the results demonstrate the feasibility of the proposed framework. This work serves as a useful resource for constructing an integrated and streamlined AIOps model management system.

Keywords:
Model management pipeline, AIOps, Microservice, MLOps

Introduction:
With the widespread adoption of microservice architecture in various enterprise information systems, the system management complexity of large-scale microservice platforms increases significantly. AIOps (Artificial Intelligence for IT Operations) has recently emerged as a promising solution to this microservice system management challenge. Under the framework of AIOps, system designers can combine machine learning (ML) and data analytic technologies to create intelligent microservice management systems. By leveraging container orchestration frameworks such as Kubernetes, AIOps-enhanced microservice systems can easily collect their real-time monitoring data streams and execution logs, and adopt machine learning models to provide proactive insights and management decisions to detect system anomalies, mediate resource allocations, and prevent potential system failures.
Under this architecture, AIOps machine learning models play a critical role in implementing decisions related to autonomous system management. It is not a simple task to train and deploy accurate AIOps models across various conditions and microservice systems. In general, there are two major concerns to be addressed.

Fast model training to adapt to the dynamics of running context. It is typically impractical to deploy an AIOps model for anomaly detection or root cause localization in a microsystem without onsite calibration. An open micro-service system may exhibit changing dynamics under different workloads. Therefore, AIOps must continually train and update models to accurately represent the dynamics of a running system. System administrators and reliability engineers monitor the outputs of the models and evaluate their performance throughout the system’s runtime. In the case of inaccurate predictions or poor decisions, it may be necessary to re-train the models with the newly collected data. Given the complexity of training a modern Deep Neural Network model for AIOps, it is difficult to run the training process with sufficient computing resources efficiently.

Encapsulation and deployment of model code as microservice. A natural way to deploy an AIOps model in a microservice system is to containerize the model code into a Docker image and then deploy the model image. To implement a model-as-a-service approach, it is necessary to streamline and automate many relevant processing steps, including model training, verification, packaging, and deployment. Clearly, Continuous Integration and Continuous Deployment (CI/CD) is the appropriate design pattern for supporting model-as-a-service in AIOps. Most existing CI/CD pipelines are designed to integrate software code, not machine learning models. To support model encapsulation and deployment, we must implement a customized model metadata description and deployment script.

To address the aforementioned issues, this paper proposes a new AIOps model management pipeline framework and implements a Kubernetes-based prototype system to validate this framework. This framework consists of the following parts:

Model training: Provides a convenient model training interface for fast model training and updating. Users can also complete model training in their own familiar way, using only the model packaging and deployment functions.

Model packaging: Used to encapsulate models into Docker images for simple deployment in microservices systems; includes interfaces for defining model dependencies and model repositories for storing models and configuration files.

Model deploying: Providing model deployment parameter configuration interface to enable automatic deployment of models on the microservice platform, as well as providing model access interface in the form of REST API.

The final outcome of this framework is an ML pipeline that users trigger by uploading or updating models in the model repository, enabling an automated packaging and deployment process for models. The contributions of this paper are threefold:

An automated model update management framework is proposed to achieve rapid and automatic model updates.
A prototype system for model update management is implemented to validate the feasibility of the framework.
The system is tested and validated, and the model packaging and deployment process is analyzed to serve as a benchmark for future system optimization and improvement.
Related work:
This section discusses the technical foundations of ML model management. With the advancement and widespread adoption of machine learning, ML applications are transitioning from ML programs to ML systems in development. Developers are beginning to focus on designing framework systems for ML application lifecycle management. It also highlights the challenges associated with providing vast and functional infrastructures and platforms to support the development and deployment of ML applications.
2.1. DevOps:
DevOps is essentially a software development mindset that aims to optimize the software lifecycle. In comparison to Agile Development, DevOps is an advanced version that encompasses the entire application development process. DevOps emphasizes continuous integration and continuous deployment, with the goal of achieving rapid iteration of application functionality to match user needs.

2.2. MLOps:
In order to establish a standard machine learning model development and deployment process, MLOps was proposed. MLOps is a practice in the field of ML that applies DevOps principles to ML systems to unify the development and operation of ML systems. The primary objective of MLOps is to shorten the iterative cycle of model development and deployment and to increase the overall efficiency of model delivery via more standardized and automated processes. The main principles of MLOps are automation and continuity. Automation necessitates the automation of all automated workflow links, from data access to final model deployment. Continuity necessitates the addition of continuous training and updating of the model based on CI/CD, i.e., whenever new data arrives, or the model’s performance declines, the model must be retrained to improve its performance.

2.3. MLOps platforms and tools:
There are currently MLOps and model encapsulation platform tools. Kubeflow is an open-source pipelined machine learning platform based on Kubernetes. It provides end-to-end lifecycle management of ML applications by leveraging TFX components such as data validation, model evaluation, and model services. Because it can be deployed in various Kubernetes clusters, Kubeflow enables ML workflows to work in any Kubernetes-based microservice platform where Kubeflow is installed. In addition to Kubeflow, MLflow is a popular MLOps platform whose primary purpose is to simplify machine learning development. The objective of this paper is to implement the primary model management functions in the simplest manner possible. Therefore, the framework proposed in this paper is simpler and easier to use than the previously mentioned platforms.

In addition, researches on MLOps have also been proposed. Satvik Garg et al. talk about tools and techniques to execute the CI/CD pipeline of machine learning frameworks in the MLOps approach, which is similar to several ideas in this paper. However, they did not perform specific practices. Mattia Antonini et al. introduce the Tiny-MLOps, a framework for orchestrating ML applications at the far edge of IoT systems. This paper investigates the management scheme for ML applications in the IoT domain, which has a similar idea to this paper but differs in the implementation scheme due to the difference in the domain. Yue Zhou et al. have also implemented an ML Pipeline Platform. However, their platform focuses on automatic model training, whereas the system proposed in this paper focuses on rapid model deployment.

There are also a number of MLOps tools that can help us streamline the model management procedure. Such as Determined, an open-source deep learning training platform that simplifies the construction of models, and Source-to-Image (S2I), a toolkit for constructing reproducible container images from source code. In addition, there are tools that can help automate the process, such as Argo, as well as tools for storing models and images, such as Harbor. In the implementation of the framework proposed in this paper, some of the functions of these tools are also applied.

The framework of the model management system:
In this study, we develop an automated model management system for model update and management requirements in AIOps under microservice architecture to enable developers to re-train and update models for deployment quickly and easily when model performance degrades. This article proposes a model management system with three primary components: model training, model packaging, and model deploying. The system architecture and workflow are shown in Fig. 1. The three system components are used to realize the training packaging and deployment functions in model management, as well as the independent management of different models through cooperation with the repository for code and images. The entire procedure is automated by CI/CD tools.
3.1. Model training component:
The model training component utilizes the synchronous data-parallel method to train models in a distributed manner. The model training implementation of data parallelism consists of a group of workers assigned to independent computing accelerators. Before each training begins, each worker is responsible for maintaining a copy of the model parameters (weights being trained) and synchronizing with the other workers. The procedure for training distributed models is described as follows.

Step 1: Each worker node computes the forward and backward propagation of the model on its mini-batch of data from the entire dataset. After completing the computation for the backward propagation pass, each worker node generates updates to the weight parameters based on the data it processed.

Step 2: All the worker nodes exchange parameter updates in order to collect all the updates from Step 1.

Step 3: Each worker node calculates the average values of the updates by dividing the number of nodes by the number of nodes.

Step 4: Each worker node applies the updated parameters to its own copy of the model.

Step 5: Return to Step 1.

In the process of distributed model training, steps 1 and 2 introduce computation and communication overhead. Specifically, step 1 results in the majority of the computational overhead. We utilize the memory’s maximum batch size to make more efficient use of the GPU and reduce the computational overhead incurred by this step. Because of this, deep learning models typically perform dense updates. Thus, model parameters are updated for each training sample, and batch size has no effect on the amount of time it takes workers to communicate updates. However, changing the batch size affects the number of training cycles described above. Therefore, we can reduce the number of process executions by increasing the batch size and reducing the total communication overhead. In general, when training a model, a suitable strategy must be selected based on the capabilities of the hardware. When there is sufficient arithmetic power, a larger batch size can be selected to improve efficiency. When there is insufficient computational power, a smaller batch size must be selected to improve performance at the expense of some efficiency. It is worth noting that using a larger batch size may lead to low accuracy issues. To alleviate this problem, it is recommended to use a custom optimizer designed for large batch training, such as RAdam or LARS.

3.2. Model packaging component:
Once the training model has been obtained, it must be packaged as a Docker image for management and deployment. To satisfy the aforementioned requirements, the model packaging part primarily consists of the following functions. The first part is the model code repository, which is used to store model code, model dependencies, model required environment, and other configuration files required for model execution during model packaging. Different code repositories are independent of one another in order to support the independent configuration of different models and in the packaging and deployment processes. The second part must conclude the definition of image packaging instructions, which primarily consists of base image information, maintainer information, image operation instructions, and container startup execution instructions. The preceding information is stored as a Dockerfile, which specifies the packaging rules for the image. The third part is the model image repository, which will be pushed to the repository via the push command after the image packaging is completed. The repository is mainly used to store the Docker images of various models. Different model versions can be managed independently so that the appropriate model version can be quickly obtained and deployed.

3.3. Model deploying component:
The model deploying component in this system is responsible for installing model images in a Kubernetes platform and converting ML models into production REST microservices. In addition, the component is responsible for scaling the model and maintaining its API functionality. The model deploying component uses Kubernetes CRD for image deployment. We configure the component’s deployment CRD using a yaml-formatted configuration file. This configuration file specifies the relevant information, such as the name, the Docker image, and the namespace of the service to be deployed. The model deploying component must create Kubernetes Pods containing the specific model image on the Kubernetes cluster based on the yaml file’s specifications.

In order to achieve a timely update of the model in the AIOps system, model monitoring, and feedback functions have been incorporated into the model deploying component. The model monitoring mainly implements the performance evaluation of the deployed model. Through regular performance evaluation, the system can detect the degradation of a deployed model’s performance in a timely manner. When such degradation occurs, the model management system’s feedback mechanism automatically retrains the new version of the model and packages it to ensure the AIOps model’s high adaptability on the microservice platform.

The implementation of the model management system:
In this section, we describe the implementation and validation of the proposed model management system. Our objective is to provide a comprehensive pipeline for model developers and users to create, utilize, enhance, and iterate their models. The two most significant processes in this pipeline operation are the automatic packaging and deployment of newly uploaded models. Therefore, in the current version, we primarily implement the automatic packaging and deployment modules and validate the packaging and deployment functions of the system. As a model training tool, we employ Determined, an open-source deep learning training platform that simplifies model construction. The prototype system implemented in this paper provides a simple and easy-to-use solution to the need for easy updates of AIOps models in microservice systems.
4.1. Model packaging implementation:
To realize the encapsulation function of the model, the model code must first be decoupled from the microservice’s logic code. Specifically, the model is contained within a separate Docker image. The model is then deployed on the microservice platform as a microservice via the Docker image and provided to the exception detection and exception root cause location module via the REST API in order to implement the function.

To be compatible with different types of models and achieve a more general model packaging tool, the model packaging tool must pre-package the model using the corresponding Python Class. The pre-packaged Python Class has two primary responsibilities, the first of which is to initialize the model and load it with trained parameters using the model configuration file and trained model parameter files. The second step is to obtain the output corresponding to the input.

After the model has been pre-packaged, it is wrapped in a Docker image using the corresponding Dockerfile, which introduces the pre-packaged Python Class through environment variables and exposes the model’s output port. Finally, the Docker image of the wrapped model is uploaded to the image repository for storage.

4.2. Model deployment implementation:
In the model deployment part, this paper mainly implements the model deployment function under the Kubernetes microservice platform. In Kubernetes, all resource objects are described by yaml files, and the Pods used to provide external services are generated by the controller object Deployment. To create the model service, we must define the corresponding controller for the model resources. The yaml file of the controller will be stored in Kubernete’s etcd database, then the scheduler will find a suitable server. Finally, the controller will create the model pod, where the docker image used for model deployment is generated by the model packaging tool and uploaded in the container image repository. The port exposed in the Dockerfile is mapped so that the model’s output can be obtained via HTTP requests. Therefore, to deploy the model, the main work to be done is to define the yaml file of the model service controller, and then based on this deployment description file, complete the model deployment task, with the following workflow.

Step 1: The yaml file is executed through the Kubernetes command-line tool Kubectl. Kubectl then commits the yaml file after communicating with the Master node of the Kubernetes cluster.

Step 2: The Kube-Apiserver on the Master node stores the yaml file in Etcd and then informs the Kube-Scheduler which worker node the Pod should be deployed on.

Step 3: Kube-Scheduler calculates the appropriate worker nodes based on the information stored in Etcd and the resources required in the yaml file and returns the results to Kube-Apiserver.

Step 4: Kube-Apiserver notifies the result to Kube-Controller-Manager, which goes on to communicate with the Kubelet on the working node to create the Pod.

Step 5: Kubelet calls the Container-Runtime of the worker node to create a container and thus a Pod by pulling the Docker image path specified in the yaml file.

Based on the above process to deploy models in the microservices platform, we realize the unified management of model call interfaces based on Seldon Core, provide similar call interfaces for different models, and differentiate different models based on the model name at the time of deployment.

4.3. Automation of the packaging and deployment process:
Continuous integration and continuous deployment are the two main processes currently used in the industry for agile development. In this paper, we use GitLab-CI, GitLab’s native continuous integration tool, to automate model packaging and deployment. The main process of implementation is to define the packaging and deployment process in the .gitlab-ci.yaml configuration file. In the configuration file, you need to define the name of the image for which the model is packaged, information about the repository where the model image is stored, information about the Kubernetes cluster where the model is deployed, and so on. It is important to note that model packaging requires Docker in Docker technology. This technology enables us to execute Docker commands within the container to complete packaging and push the model to the container. The aforementioned parameters are only required to be configured upon initial use. For subsequent model updates, simply submit the trained model files to the GitLab repository, and the pipeline will be automatically triggered, with the Gitlab CI Runner performing the corresponding model packaging and deployment tasks according to the configuration file for both phases.

Experiments:
Based on the self-built Kubernetes cluster, we have conducted experiments to validate the prototype system. Due to the fact that the entire system is not yet fully implemented (We currently implement the following functions: storage of model code and model images, automatic packaging, and automatic deployment of models), we only conducted preliminary tests on the system’s primary functional components.
5.1. Runtime environment:
The model was trained on a physical machine equipped with two Intel Xeon Silver 4114 CPUs, two Tesla V100 graphics cards, 16GB of video memory, and 128GB of memory. We deployed a Kubernetes cluster with ten nodes on the local service machine; the Kubernetes version corresponding to the cluster is v1.20.6. We manage the Kubernetes cluster based on Rancher. We deployed Harbor as a repository for models and model images in our cluster. We developed Gitlab as a repository for model and configuration files. Additionally, we deployed Gitlab Runner in our cluster and configured it for packaging and deploying models.

5.2. Experimental setup and results:
We utilize the MNIST dataset to train the image classification model, which can be found in this Github repository, to test the model training components. We have configured “const” and “adaptive” configuration files for the model. In hyperparameter search, the former is used for training and testing a single model, while the latter is used for training and testing multiple models. In the training experiment of a single model, our model learning rate is 0.5, and the model contains three layers. Dropout is 0.25 for layers 1 and 2, and 0.5 for layer 3. In this experiment, eight trainings of a single model were performed, and the average training duration was recorded. The validation loss changes of the model in a single model training are shown in Fig. 4, and the final validation loss value is 0.044, validation accuracy is 0.986.

We conducted eight hyperparameter search experiments and ensured that at least one experiment possessed the same validation precision as a single model training experiment. The model structure of the hyperparameter search experiment is the same as that in the single model experiment. The difference is that the corresponding parameter of the hyperparameter search experiment is a variable range, rather than a fixed value as a single experiment. To compare single-model training and multi-model training, the training process of the models is illustrated and compared using loss curves. In the experiments, since the training loss curve and the validation loss curve are relatively similar in terms of trend, we use the validation loss curve as a representation here for simplicity. The validation loss curve of the best hyperparameter search experiment is shown in Fig. 5, and the result of the complete hyperparameter search experiment is shown in Fig. 6. One can see that the best validation loss and accuracy value among the eight hyperparameter search experiments is 0.056 and 0.982, and it took 227 s in total.

Training time and validation loss results for a single experiment and a hyperparameter search experiment are displayed in Table 1. It is evident from the results that the average training time for multiple hyperparameter search experiments employing parallel training is less than that of a single experiment. Due to the fact that hyperparameter search only requires one experiment to achieve the same level of precision as a single model experiment, some experiments may conclude prematurely. The fact that eight parallel experiments can be completed in 250 s, which is significantly less than the time required for serial completion, demonstrates the feasibility of the parallel training we have achieved.

In addition, we perform experimental validation of the model packaging and deployment tools described in this paper. As test objects, we chose three typical AIOps models for various scenarios, namely ARIMA model for service metrics prediction, a Transformer-based anomaly detection model (obtained from an open-source transformer model), and a graph neural net-based fault root cause location model. We created a model repository based on the three previously trained models, configured the packaging and deployment parameters, carried out the packaging and deployment process, and logged the time spent on each step. The specific results are shown in Table 2.

Among the total time consumed, the packaging process accounts for more than 80%, while the deployment takes less than 20%. This is due to the fact that the model package process requires downloading and installing various dependencies, completing the Docker image build process, and then pushing the image to the storage repository. These operations often take a lot of time. Only the deployment request and configuration parameters must be submitted to the service cluster during the deployment process. Since the model images are stored locally, this results in shorter image pull times. Therefore, deployment time will be significantly less than packaging time. Additionally, we observe that packaging and deployment times for various models vary. Different models necessitate distinct dependencies, which is the primary cause of this phenomenon. The greater the number of required dependencies, the longer it typically takes to configure the dependencies, and network fluctuations can result in time variation.

The exact execution time for each step of the packaging and deployment process has been recorded, and the results are displayed in Fig. 7. In the packaging stage, Environment Preparation and Job Script Execution are the two most time-consuming steps. In contrast, in the deployment stage, time is primarily consumed in Environmental Preparation. We also calculated the percentage of execution time consumed for each step, and the results are shown in Table 3.

According to the results, Job script Execution consumes more than 60% of the image packaging process. And Environmental Preparation takes up more than half of the deployment process. Environment Preparation creates and runs the runner pod, including image pulling and image launching, which is primarily related to the Kubernetes cluster network conditions. Job Script Execution performs environment configuration, dependency installation, and Docker image building and deployment based on the configuration information in .gitlab-ci.yaml. By analyzing the time used for the above processes, we can optimize the system. In the Environment Preparation stage, we can create a shared container cache for the hosts in the Kubernetes cluster, thus skipping the image pull step to reduce the time spent. It is also possible to build base images where common model dependencies are pre-installed and common environments are configured, thus reducing the number of dependencies and environments that need to be installed and configured for model packaging, in order to achieve time optimization for the Job script Execution.

According to the experimental results, the model management system implemented in this paper is capable of completing the tasks of automatic packaging of models and deployment of models on Kubernetes clusters in a relatively short amount of time. Moreover, the aforementioned process can be initiated automatically after the initial configuration without manual operation, which greatly improves the convenience and updates efficiency of model update and demonstrates the prospect of the model management framework proposed in this paper.

Conclusion:
In this paper, we propose an AIOps model management prototype system based on Kubernetes for the operation model update problem caused by the dynamic nature of microservice systems. The system consists of three parts model training, model packaging, and model deployment, which encompass the entire model updating process. We applied Gitlab and continuous integration technology for the implementation of the prototype system. Based on the Gitlab repository for model and configuration file storage, based on Gitlab-CI to automate the model packaging and deployment process. We tested the system’s availability by updating and deploying the operation and maintenance models in multiple experiments. The results demonstrate that the system we implemented is able to automatically perform model packaging and deployment on the Kubernetes cluster. In addition, we conducted a statistical analysis of the time spent on each stage of the packaging and deployment process and identified the primary factors influencing the efficiency of model packaging and deployment, which provides a foundation for subsequent optimization of the entire system.
In the future, we will further improve the whole system, further implement and test the relevant functions that have not yet been implemented. Additionally, we will test our system on more models. If possible, we intend to test our system in a real production environment to continuously test and optimize it in order to create a model management system with a broader scope of application.