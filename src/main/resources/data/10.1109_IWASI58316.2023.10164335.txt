A Cloud-Edge Artificial Intelligence Framework for Sensor Networks
Giuseppe Loseto*, Floriano Scioscia†, Michele Ruta†, Filippo Gramegna†, Saverio Ieva†, Corrado Fasciano†, Ivano Bilenchi†, Davide Loconte†, Eugenio Di Sciascio†

Abstract—Internet of Things devices allow building increasingly large-scale sensor networks for gathering heterogeneous high-volume data streams. Artificial Intelligence (AI) applications typically collect them into centralized cloud infrastructures to run computationally intensive Machine Learning (ML) tasks. According to the emerging edge computing paradigm, instead, data preprocessing, model training, and inference can be distributed among devices at the border of the local network, exploiting data locality to improve response latency, bandwidth usage, and privacy, at the cost of suboptimal model accuracy due to smaller training sets. The paper proposes a cloud-edge framework for sensor-based AI applications, enabling a dynamic trade-off between edge and cloud layers by means of: (i) a novel containerized microservice architecture, allowing the execution of both model training and prediction either on edge or on cloud nodes; (ii) flexible automatic migration of tasks between the edge and the cloud, based on opportunistic management of resources and workloads. In order to facilitate implementations, a scouting of compatible device platforms for field sensing and edge computing nodes has been carried out, as well as a selection of suitable open-source off-the-shelf software tools. Early experiments validate the feasibility and core benefits of the proposal.

Index Terms—sensor networks, cloud-edge intelligence, machine learning, microservice architecture

I. INTRODUCTION AND MOTIVATION
Sensor networks based on Internet of Things (IoT) devices allow increasingly large deployments to monitor natural and human-made environments and processes. The scale and variety of gathered data streams is rising consequently. In order to make sense of sensor data, Artificial Intelligence (AI) provides Machine Learning (ML) approaches and models which can be trained on larger and larger amounts of samples to improve accuracy and generalization. Even after data stream preprocessing and aggregation within device clusters of the sensor network, applications typically need to upload information to cloud computing infrastructures, where model training and prediction tasks can be executed at massive scales.

This work has been supported by projects TEBAKA (TErritorial BAsic Knowledge Acquisition), funded by the Italian Ministry of University and Research, and BARIUM5G (Blockchain and ARtificial Intelligence for Ubiquitous coMputing via 5G), funded by the Italian Ministry of Economic Development.

In recent years, the edge computing paradigm exploits the increasing computational capabilities of devices at the edge of the sensor networks –such as cluster-heads, sinks, and gateways– to run processing tasks, in order to reduce communication latency and bandwidth usage, improve response times, and protect data privacy by keeping information within the local network. Cloud-Edge Intelligence (CEI) is one of the most relevant trends in edge computing, aiming to assign ML model training and prediction tasks either to edge or cloud nodes, based on application requirements, device status, and network conditions. Designing collaborative distributed systems for sensor networks able to support Cloud-Edge Intelligence requires facing the complexity of dynamic resource and service management across edge and cloud layers, therefore mature approaches and practical solutions are not available yet.

The paper proposes a CEI framework for sensor networks, inspired by the Osmotic Computing paradigm. It enables dynamic orchestration of containerized microservices implementing individual logical components of a complete cloud-edge infrastructure for sensor data collection, preprocessing, storage, ML model training, inference, and analytics. Automatic deployment and migration are performed in an elastic and opportunistic fashion, considering application workload requirements as well as device status and network topology changes due to device mobility, energy supply outages, or link congestion. While cloud computing can process larger amounts of sensor data and therefore train more accurate ML models, edge computing can avoid sensor data upload through the Internet, therefore reducing bandwidth usage, bypassing communication latency, and preserving data privacy. The proposed approach aims at providing a dynamic trade-off between the respective benefits of the edge and cloud layers.

In order to facilitate practical implementations of the proposed general framework, a thorough technological scouting of hardware and software off-the-shelf components has been performed. Compatible hardware device platforms for sensing nodes based on Micro-Controller Units (MCUs) and for edge computing nodes based on Micro-Processor Units (MPUs) have been identified. At the same time, open-source software tools and libraries have been selected as possible implementations of the logical components of the framework. This has allowed building a small-scale prototypical testbed, which has been used to carry out preliminary experimental tests on a well-known dataset in the public domain for a regression ML problem in an industrial scenario. Outcomes basically show the ability of the proposed framework to achieve a variable trade-off between training set size and model accuracy with respect to training and inference time as well as communication latency.

The remainder of the paper is as follows. Section II describes in detail the framework architecture and its individual components, along with compatible hardware platforms and software tools. Experiments are reported in Section III, while Section IV discusses related work before conclusion.

II. FRAMEWORK ARCHITECTURE
The architecture shown in Figure 1 serves as the foundation for the proposed framework. Microservices are packaged in containers and opportunistically deployed to devices. Container orchestration and deployment to available devices in the reference architecture adhere to Osmotic Computing. In this paradigm, service orchestration methods must consider the dynamic requirements of sensor network infrastructure and applications: the former include load balancing, dependability, and availability, while sensing and actuation capabilities, context awareness, topology constraints, and Quality of Service (QoS) are taken into account among the latter. As a consequence, it is essential to be able to handle the migration of microservices between the cloud and edge layers bidirectionally.

In the cloud, datacenters accommodate various service kinds which are built in accordance with high-level application needs. The edge layer, located between IoT sensors in the field and the Internet, consists of cluster heads, data sinks, and gateway nodes able to process generated data. In conventional architectures, processing tasks on edge nodes were limited to data gathering from sensors in the environment with simple data preparation procedures, setting the stage for later mining activities. Until recently, more sophisticated processing was considered as either minimally viable or not possible at all, due to computing resources limitations.

The proposed framework exploits the increasing capabilities of edge devices in order to execute prediction tasks using pre-trained models as well as complete processing pipelines involving data preparation, feature extraction, training, and prediction. The framework comprises various logical components deployed to heterogeneous devices, allotting different types of tasks between the cloud and edge layers based on differences in requirements and capabilities. Since edge devices usually have lower processing and memory resources, model training tasks are limited to smaller datasets with respect to cloud deployments. Consequently, model training and prediction are assigned to edge nodes when suboptimal accuracy is an acceptable trade-off with respect to other requirements, such as reducing response latency or defending privacy by avoiding transmission through the Internet.

In order to facilitate practical implementations of the devised framework, the following subsections provide details about logical components and reference target device platforms, respectively.

A. Logical components
The proposed framework architecture relies on a number of lightweight, loosely connected microservices acting as logical components, each endowed with specific responsibilities. They support a range of composition patterns and fine-grained scalability to satisfy the requirements and constraints of applications. The defined services are outlined in what follows.

Local Storage: Provides temporary local storage for data collected from field sensors and IoT devices. Closeness to data processing microservices is desirable for both latency and bandwidth optimization reasons, and centralized data storage should be avoided.
Data Processing: Preprocesses data in preparation for ML model training tasks. In large-scale heterogeneous sensor networks, this task is usually split among edge nodes to distribute the computational load and take advantage of data locality by accessing the nearest Local Storage service instances directly. This lowers bandwidth usage and latency, as well as risks related to connectivity and power supply unreliability in edge devices.
Data Stream Management System (DSMS): Serves as the platform’s Message Broker (MB), adopting event-driven asynchronous communication to forward data and event streams from the edge to the cloud and vice versa via the publish/subscribe pattern, which specifically fits scalable loosely-coupled microservice architectures. The data and control planes of the network are logically separated by adopting different topics for messages in the publish/subscribe communications. Care has been taken to make the DSMS interoperable with the most popular communication protocols for IoT-based sensor networks, including Message Queuing Telemetry Transport (MQTT) and Constrained Application Protocol (CoAP).
Data Producer and Data Consumer: Respectively sends and receives preprocessed data through the Message Broker. The Data Producer microservice is typically deployed on edge nodes, while the Data Consumer on cloud nodes.
Edge Intelligence: Runs classification and regression tasks on edge devices. Exploiting Local Storage data, this microservice can also train and validate models, providing greater privacy and security, as the transit of sensitive data through the Internet can be prevented, as well as reduced bandwidth usage and latency, since data upload to and model or prediction download from the cloud are not required. Distributed learning across several Edge Intelligence instances can handle big real-time data streams through horizontal scalability. Each Edge Intelligence model is trained independently of the others.
Cloud Intelligence: As a counterpart of the Edge Intelligence service, it executes model training and prediction on sensor network data streams collected at the cloud layer from various Data Producer instances. This component enables continuous model improvement through a feedback loop: a less accurate model can be trained and used on edge devices from a subset of the whole sensor network, while a more accurate model is trained in the cloud by collecting larger amounts of data. The latter can be sent to Edge Intelligence nodes to replace their local models, and the process may be repeated as new data is gathered. As for Edge Intelligence models, the training process is independent from the rest of the network.
Data Analytics: Performs business intelligence analytics and visualization on the data collected at the cloud layer.
Orchestrator: Employs a container-based strategy to manage all the above microservices. It is able to migrate microservices from the edge to the cloud and vice versa in real time, according to the availability of resources, e.g., when network topology changes, services are under excessive load or devices fail.
B. Hardware platforms and software tools
The heterogeneity of the aforementioned logical components allows the proposed framework to leverage various types of nodes based on Micro-Processor Units (MPUs) and Micro-Controller Units (MCUs) for dispatching tasks at different levels of the local network. MCUs execute programs stored on Flash memory embedded on chip, enabling faster start-up times and short bursts of computation. They are usually exploited for real-time tasks like data acquisition. On the other hand, in MPU architectures the processor accesses an external non-volatile memory for program and data storage, which provides greater memory space but requires a longer start-up time, during which the code must be loaded into an external Dynamic Random-Access Memory (DRAM) before starting execution. DRAM and non-volatile memory typically range from hundreds of megabytes to gigabytes, while the most capable MCUs on the market currently have a maximum of 2 MB of program memory and hundreds of kilobytes of embedded Static Random-Access Memory (SRAM).

Both categories of devices are typically supplied as development boards to support software and firmware development, as well as standalone chips for integration into final products. Table I reports a list of reference Commercial-Off-The-Shelf (COTS) development boards for embedded micro-controllers supported by the proposed framework, while Table II reports a list of reference MPU-based single-board computers.

In the architecture described in Section II, MCUs are employed in sensing devices for collecting data and performing basic data processing tasks. Edge nodes are typically more capable devices, which use MPUs and enable the provisioning of resources for executing containerized microservices. By managing container images compiled for the various MPU architectures, each microservice can be deployed to multiple boards having different hardware characteristics, allowing applications to scale horizontally in a cost-effective fashion based on the current workload. According to the Osmotic Computing paradigm, the various task types are dynamically assigned to physical nodes taking their computational and memory resources into account: for instance, devices with limited processing power can serve as storage modules, while boards with faster CPUs can be used for more intensive computation.

With regards to software components, a comprehensive market research has been conducted to identify off-the-shelf technologies that are suitable to implement each microservice. Various factors have been assessed, including functionalities, technical features, costs, and licenses. The proposal aims to reduce time and effort required for platform development and integration, while enabling more robust, flexible and scalable systems. Table III outlines a recommended mapping between the proposed logical microservices and available software components.

The openBalena platform is the foundation of the architecture, enabling the management of containers on fleets of devices, by means of capabilities including application container configuration, update distribution, network parameter sharing, and container image deployment to devices using various strategies. In order to integrate with the platform, devices must run balenaOS, a lightweight UNIX-like operating system based on the Yocto project, which has been customized to run application containers on single-board computers. All the devices listed in Table II are supported. For container management, balenaOS includes balenaEngine, a Docker-compatible daemon optimized for application service images, containers, and volumes deployed on resource-constrained devices. This tool overcomes common virtualization problems related to constrained scenarios, such as resource overhead and lack of hardware support, and is available for various device types and different processor architectures.

Apache Kafka is a distributed DSMS for event streaming, enabling publish/subscribe communication among devices and applications with high throughput and horizontal scalability. Kafka is exploited not only to send and receive streams of event data, but also to send event feedbacks to edge nodes and to facilitate the exchange of ML outputs between cloud and edge modules. Each microservice can use the Kafka Producer/Consumer API to produce (send) and consume (receive) data on specific topics. Bindings for both APIs are available in various programming languages, but Python is recommended for its seamless integration with other Python-based platform components for Data Processing, Edge/Cloud Intelligence, and Data Analytics. TensorFlow and Keras have been chosen for implementing machine learning tasks in both edge and cloud deployments, and Streamlit to create interactive web applications capable of plotting sensor data, highlighting basic statistics and patterns, supporting exploratory data analysis, and visualizing performance results of ML predictive models. Finally, the Redis in-memory key-value data store has been chosen for data gathering from sensors and IoT devices: among features making it suitable for edge computing scenarios, there are low CPU and memory requirements, lightweight data structures effective for time-series data, a simple data model which can be adapted to a range of data sources and applications, and append-only storage options optimized for Flash memories typically found in MCU-based devices.

III. EXPERIMENTAL EVALUATION
Following the architecture described in Section II, a prototypical testbed has been implemented to demonstrate the feasibility of the proposal and to conduct a preliminary experimental campaign. The testbed consists of the following components:

Two Edge nodes: a Raspberry Pi 3B (Edge 1) and a 3B+ (Edge 2), running Tensorflow in a balenaOS container for ML tasks on inbound data.
Message broker (MB): a Raspberry Pi 4B, running an instance of Apache Kafka in a balenaOS container.
Cloud node: simulated via a notebook computer running Docker desktop, able to perform ML tasks via Tensorflow.
The edge nodes are interconnected through an IEEE 802.11 wireless link. The cloud node and the Message Broker communicate through an Internet connection.

The selected ML task is linear regression on a well-known dataset (N = 737453 samples, size 160 MB), carried out by training a multi-layer perceptron regressor with 5 hidden layers, 200 neurons per layer, and Rectified Linear Unit (ReLU) activation function. The network is trained for 10 epochs using the Keras implementation of the Adam optimizer, with default parameters and Mean Squared Error (MSE) loss function. The choice of this model is based on its ability to deliver satisfactory prediction performance while being lightweight enough to run efficiently on devices with limited resources, such as the MPUs listed in Table II. The relatively small size of the network also allows exploiting some of the MCUs in Table I, by means of appropriate model size reduction techniques such as weight pruning and quantization.

Performance tests have been carried out in the context of the following workflow, which can represent a typical interaction in Cloud-Edge Intelligence scenarios:

When the testbed is first deployed, edge and cloud nodes independently train a ML model. The cloud node uses the full dataset to simulate the aggregation of data from all sensor nodes, while each edge node only uses a subset of it in order to simulate local data availability.
After completing the training process, they communicate with the message broker by sending a message on a control Kafka topic.
Once training is complete on all the nodes, the MB simulates a request for inference on specific sensor data by advertising it on a data topic.
The edge and cloud nodes perform inferences on the sensor data and publish results back to the data topic.
Table IV reports on training times and validation metrics (coefficient of determination R² and MSE) for different dataset sizes on the Raspberry Pi 3B+ edge device. The findings suggest that there exists a reasonable trade-off between model accuracy and training time, which aligns with previous studies stating that fractional datasets do not significantly reduce prediction accuracy as long as the data distribution is representative of the overall dataset. Moreover, even with a substantially smaller sample size, such as N/8 or N/16, the model retains most of its predictive capability. This implies edge nodes can perform inferences effectively with models trained locally.

Table V illustrates the following metrics:

Inference time: the time it takes to predict the regression value for a single sample;
Communication latency: the time between sending and receiving messages between the different components of the architecture in the prediction phase. It comprises two components: from message broker to the respective intelligence node (MB-to-I), and from intelligence node to message broker (I-to-MB);
Turnaround time: the overall time between the issuing of a prediction request on a data sample and the reception of the response, as measured by the message broker.
Although the cloud offers a significantly lower prediction time, edge nodes exhibit considerably lower communication latencies. In several application scenarios based on sensor networks, the availability of a stable connection to the cloud is far from guaranteed: available bandwidth may decrease due to concurrent activities of large numbers of devices, a condition not replicated in the experimental setup. In such cases, shifting training and prediction tasks to the edge may enhance the overall system performance by reducing the outbound network pressure.

IV. RELATED WORK
In Cloud-Edge Intelligence AI models should be collaboratively executed between the edge and the cloud to minimize both end-to-end latency and energy consumption when compared to relying solely on local execution. CEI can be classified into seven levels, which represent a spectrum of trade-offs between the amount of data transferred, latency, energy consumption, and data privacy. The Cloud Intelligence level involves a fully centralized approach with no data privacy benefits, while the All on-device level provides maximum data privacy but requires powerful end devices, which is unfeasible in large-scale pervasive sensor networks. The levels in between provide a range of options for optimizing the trade-off, depending on the specific application and the available resources. In-Edge co-inference is the preponderant approach in literature, where ML models are trained on the cloud and then used for prediction at the edge. GEM-Analytics is an example of Cloud-Edge AI platform developed for energy management purposes, using cloud-trained and validated models that are periodically sent to edge nodes for routine operations in power plants.

The Osmotic Computing (OC) paradigm can be useful to pursue more dynamic CEI trade-offs. Notable system examples in this regard include: a trust management framework for Pervasive Online Social Networks (POSNs) using OC for offloading computation among multiple users; Apollon, a platform for pollution monitoring that utilizes EI for data preprocessing from heterogeneous mobile and IoT devices in Smart Cities; the OC architecture for a smart classroom that uses deep learning models for entity recognition, handwriting recognition, and IoT device control; RAPTOR, an osmotic platform based on the R statistical computing environment to allow flexible data analysis application creation, deployment, and integration through microservices orchestration; the OC-based Message-Oriented Middleware (MOM) for IoT environments; En-OsCo, an energy-aware resource management approach based on monitoring of edge data centers and heuristics to optimize the dispatch of services for incoming workloads; Osmosis, an OC framework focusing on microservice deployment across cloud, edge, and IoT environments, with the ability to migrate MicroELements (MELs) and their associated microdata, which was used to build a prototypical distributed healthcare system; the architecture, which uses an edge-cloud platform for migrating and scheduling services across multiple servers to support distributed mobile augmented and virtual reality applications, with low latency, robustness, and tolerance as key requirements. With respect to these approaches, however, the framework proposed in this paper is the only one combining osmotic orchestration of containerized microservices and Cloud-Edge Intelligence to support ML model training and prediction tasks on both the edge and the cloud.

V. CONCLUSION
This paper has introduced a Cloud-Edge Intelligence framework for Machine Learning applications leveraging IoT-based sensor networks. It is inspired by Osmotic Computing and exploits a microservice architecture to containerize data gathering, storage, preprocessing as well as model training and inference tasks. This allows dynamic orchestration and deployment of microservices either to edge computing or to cloud computing nodes, in a flexible, scalable, and opportunistic fashion, based on application requirements and resource availability. Framework modularity has facilitated the identification of reference hardware platforms for sensing nodes and edge computing devices, as well as of off-the-shelf open-source software tools implementing individual logical components of the overall architecture. Early experiments on a small-scale testbed with a public domain dataset for an industrial ML scenario have shown the approach enables a combination of simpler models trained at the edge with smaller amounts of data and faster prediction response time, with models trained on cloud nodes on larger amounts of data for greater accuracy.

Future work includes expanding the testbed implementation with a real sensor network and a real cloud computing infrastructure, in order to test all performance and scalability aspects in a challenging scenario. Furthermore, the framework itself can be extended by exploiting semantic matchmaking to perform a more fine-grained assignment of tasks to devices, as well as by supporting semantic-enhanced IoT-oriented ML approaches to achieve higher explainability of models and prediction outcomes.

