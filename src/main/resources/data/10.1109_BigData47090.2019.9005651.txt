Abstract— In this paper, we propose a new framework, DeepLite, for real-time deep learning on the edge. In DeepLite, a network of multiple deep learning models is designed to conduct the context-aware inferencing using real-time deep learning technologies. Comprehensively, DeepLite has several innovative concepts as follows: 1) an inference network of deep learning models/containers, 2) invocation of a new model based on the output of the previous model’s inferencing, 3) intelligent containers for models, 4) plug-and-play model/container, and 5) microservice architecture on the edge. DeepLite has been evaluated via a case study, NeighborNets which was based on an inference network of several deep learning models deployed to edge devices for computer vision, e.g., object detection algorithms to compute a diverse range of aspects, such as house types, level of greenery, house age, traffic conditions, and the types of recreational facilities.

I. INTRODUCTION

With a vast number of mobile/IoT devices, leveraging smart offload to clouds, distilled knowledge among thousands or millions of endpoints introduces new challenges and opportunities in the immense cyber-physical systems/IoT ecosystems. Due to limited capabilities of mobile /IoT devices, it may not be trivial to build and deploy large deep learning (DL) models on such devices. Even if possible, they may not offer real-time responses to users. Existing DL lightweight solutions, such as TensorFlow Lite and TensorFlow Mobile, mainly focus on providing optimized DL models that were designed for production environments such as Android or iOS. However, the solutions available so far are not sufficient to account for real-time deep learning applications.

Most real-world applications need more comprehensive solutions that cannot be handled by a single DL model. A comprehensive model is too big and complex for edge computing. Also, the models available to deep learning researchers are so generic so that they are challenging to apply in real scenarios. High-performance DL inference has demonstrated several real-time deep learning applications using the NVIDIA TensorRT platform which supports an optimizer and runtime for low latency and high throughput. In order to build real-world and real-time DL applications, we are motivated to use the aforementioned high-performance deep learning platforms and frameworks. In particular, we are interested in building a neighborhood application with real-time DL models and cutting-edge technologies.

Deep learning for neighborhoods is a particularly exciting yet challenging topic, as neighborhoods are inherently diverse and collective in terms of age, culture, safety, environmental impact, and population. Practically, it may not be possible to perform one-time learning to yield an accurate evaluation of all the aforementioned significant factors as the network may become uncontrollably large and complex. Therefore, in order to efficiently break down the intrinsic complexity and efficiently leverage the multifaceted characteristic of neighborhoods, our team’s unique approach is to split the analysis of neighborhoods into a collective list of factors, carefully examine each factor via deep learning, and finally construct a robust network from this extensive collection of models through transfer learning.

In this paper, we propose our solution, DeepLite, which is designed as a lightweight DL platform. DeepLite aims to provide real-time DL applications and edge computing capability with adaptive sensing and localized understanding of domains. The main contributions of this paper include (1) DeepLite’s ability in real-time deep learning for real-world applications, (2) DeepLite’s architecture which seamlessly works with the cloud for deploying the DL models, (3) An inference network for distributed and collaborative inferencing capability of models on the edge, and (4) NeighborNets for dynamic inferencing demonstration based on users’ contexts in a real-world application.

II. RELATED WORK

A. Mobile Deep Learning

There has been an increasing demand for mobile applications for small networks or dynamic networks in deep learning. There have been several deep neural architectures to strike an optimal balance between accuracy and performance. The lightweight DL was achieved using three types of layer compression techniques, namely: weight compression, convolution compression, and adding a single layer. Weight compression is the primitive technique used to create a lightweight model. MobileNet-V1 and Shufflenet used a specific convolution compression technique in their architecture: depth-wise separable convolutions and point-wise group convolutions, respectively. As an extension of the previous works, MobileNet-v2 added a new layer, an inverted residual layer, with a narrow bottleneck to create a lightweight model. In NasNet Mobile, Neural Architecture Search (NAS) was proposed with reinforcement learning for knowledge transfer. In general, architectural changes are typically essential to achieving a lightweight model. In the DeepLite framework, we have obtained a context-aware model through a network of models.

B. Real-Time Deep Learning

Real-time DL plays an important role in human activity recognition (HAR) in people’s daily life. The HAR applications are well-integrated with DL technologies and real-time streaming data from edge devices. Successful HAR applications include home behavior analysis, video surveillance, gait analysis, and gesture recognition. Some existing applications that were developed in mobile settings, e.g., speech recognition, face recognition, emotion recognition, and activity recognition, mainly focus on a cloud-based inferencing model, rather than IoT-based model. A range of innovative approaches, such as localized or context aware deep learning with IoT and mobile devices, has not been fully explored. DeepLite will provide significantly enhanced agility, response, and availability for small mobile and IoT devices. In this way, DeepLite will substantially reduce data traffic, decision speed, response rate, and real-time actuation/execution. DeepLite will play a key role in exploring DL in mobile contexts and will lead to the development of future innovative intelligent IoT and mobile apps.

C. Deep Learning with Edge Devices

Unlike TensorFlow Serving which provides high-performance inferencing services on the cloud, Alsing presented a deep learning model for real-time object detection using transfer learning and adaptation for mobile use. Li et al. introduce edge-based deep learning for IoTs. Because of the limited capability of the edge platform, they proposed an optimization strategy for IoT deep learning applications on edge. Lane et al. presented mobile IoT devices with mobile systems on chips (SoCs) with a new acceleration engine, i.e., DeepEar. DL applications with IoT devices have been deployed in dynamic and complex environments such as disaster management.

D. Deep Learning for Neighborhoods

Gebru et al. estimated the demographic makeup of neighborhoods in the United States, and Apte et al. analyzed environmental exposures worldwide by utilizing DL and Google Street View (GSV). Weichenthal et al. and Zhang et al. integrated multiple data streams using deep convolutional neural networks to support exposure science and environmental epidemiology. They claimed DL models could be widely used for predicting the future/past of land use, traffic management, behavior monitoring, and surveillance systems. Andersson et al. investigated the prediction of crime rate using street-level images and siamese convolutional neural networks. Similar to our work, Helbich et al. utilized GSV images and DL to extract metrics of green and blue space and inversely associated with depressive symptoms among the elderly. These studies presented the combination of GSV, and deep learning would be instrumental in automated environmental assessments of physical streetscapes and applicable to extensive epidemiological studies. Together, they will contribute tremendously to the promotion of healthy urban environments and mitigate the impact of rapid urbanization.

III. PROPOSED MODEL

A. Architecture
Adoption of Docker containers is booming for the following reasons: it is lightweight, it offers faster boot time when compared with virtual machines, it has self-contained dependencies which enable portability across multiple platforms (e.g., public cloud, private cloud, or edge). We adopted Docker containers as the runtime environment to apply dynamic updates to the DNN models deployed on edge and Cloud Kubernetes cluster. Our architecture enables dynamic packaging of the DNN models as Docker images that can be deployed rapidly as Rest APIs either in the cloud or on the edge.

The local environment accommodates model parameter extractor service to validate and upload the trained model weights and framework dependencies to the remote AWS S3 object storage service. The container builder service hosted on AWS EC2 is triggered through the S3 event object push notification. After the trigger, the intelligent container builder builds and deploys the container image from the model artifacts that were pushed into the S3 bucket. By providing the on-demand dockerization-as-a-service solution for the CNN models, we streamlined the DevOps process for the rapid deployment of models either in the cloud or edge. We adopted Kubernetes, an open-source container orchestration platform for edge and cloud DNN containerized model deployment. We provisioned the Kubernetes platform on AWS EC2 instances using Terraform and Kubeadm. Dynamic container builder and deployer services trigger Ansible, open-source agentless DevOps automation tool built on Python. Ansible custom modules and roles generate dynamic dockerfile, Kubernetes deployment manifest files for the Cloud and edge platform. Ansible provides a Jinja2 templating engine to produce dynamic configuration files on the fly.

Our edge environment accommodates the NVIDIA Jetson Nano with 4 GB memory, NVIDIA Jetpack 4.2.2, Samsung 128 GB MicroSDXC Evo select memory card. Deployer service provisions the Kubernetes cluster and deploys the model as a Restful service on the edge. We provisioned NVIDIA container runtime that containerizes the edge services.

B. Inference Network: Real-Time Deep Learning

The overarching goal of the inference network is to support real-time DL in terms of (i) inferencing with real-time streaming data, (ii) inferencing on edge, (iii) distributed and collaborative inferencing with multiple models/containers, and (iv) a dynamic and automatic inference network. The inference network is defined with a vertex set V, containing a node (i.e., inferencing unit is a pair of model/container) for DL inferencing and an edge set E, containing a directed edge for an inferencing request through a REST API call. The edge between nodes would be established according to the output of the inferencing node. Domain experts will design the inference network, and then the order of inferencing in the path can be determined automatically during run time according to a user’s context. Specifically, if a current model (the source model) has three potential outputs, three edges are generated from the source model to the next model (the target model). The domain expert will specify the target model for the contextual condition (based on the output of the source model and contextual information from the edge).

The primary goal of our inference network is to assess the neighborhood in various contexts. The network is dynamic, automatic, and coupled with the inference output of source models. It supports collaborative inferencing through multiple models to assess various components of a real-world neighborhood. Our architecture is specific to neighborhood assessment. The inference network is specially designed for the edge computing environment where the collaboration between the models can be established. This proposed architecture is very similar to service composition in a microservice architecture with fine-grained services and lightweight protocols. We mainly focus on optimizing the inferencing through the inference network. A node in the network is a model deployed as a REST API through a container, and an edge represents a sequence of nodes in which a current node calls the next node as an invocation of the RESTful web service. Consequently, the coordinator receives responses from the service provider.

The microservice architecture capability of the edge computing will maximize the number of complex inferencing tasks in the edge computing environment. As a case study, we create an inference network of 5 different CNN models for our practical applications, NeighborNets. The NeighborNets application is a sophisticated deep learning application that has a dynamic and context-aware of deep learning capability. The inference network can be modified to add or delete nodes. Inferencing services are dynamic and adaptable to users’ contexts. We have deployed the NeighborNets’ applications to the NVIDIA Jetson Nano, which is a real-world edge computing environment.

C. NeighborNets: Neighborhood Application

The overarching goal of the NeighborNets is to analyze the neighborhood efficiently and accurately at which users, homeowners, and authorities will be able to identify their community’s strengths while at the same time, examine the potential room for improvement based on the score given to each category evaluated by the network. For simplicity and consistency, we carefully selected some physical and observable conditions from the neighborhoods as representations of each evaluation criterion. For instance, a diversity evaluation will be determined based upon the existence of unique cultural objects within the neighborhood, such as flags, languages of restaurants’ signs, paintings, sculptures that suggest whether the neighborhood may belong to any particular ethnic group. Additionally, for environmental quality and impact, our network will directly look at the greenery level of the community as a whole and, via object detection, devise an appropriate metric that can conclude whether this community focuses on a green lifestyle or not.

With the NeighborNets mobile interface, users can navigate to their preferred location from a top-down perspective with a satellite-view Google Map, in which the scenery and landscapes of the neighborhoods are more apparent and realistic compared to the standard default map view. Once clicking any specific spot on the map, the system will automatically display information relevant to the selected area, such as multiple street view images of the neighborhood, important nearby places (e.g., schools, hospitals, museums, tourist attractions) along with their corresponding sentiment metrics (age, diversity, greenery, safety, traffic conditions, and recreational level). Based on these convenient visual and statistical features, users will be able to surf the neighborhood more quickly while grasping a fuller and more comprehensive analytical understanding of the neighborhoods.

D. End-to-End Workflow for Model Construction

DL technologies can be applied to numerous domains such as computer vision, natural language processing, and speech recognition. We have determined to utilize computer vision models for the NeighborNets project. There are several types of computer vision models, such as classification, detection, and segmentation. Classification is classifying an image into a specific category. Object detection is locating the position of the object in an image while segmentation is masking the actual object in an image. In this paper, we concentrated on computer vision models in classification and detection domain for neighborhood sentiment analysis. As a matter of face, data plays a major role in building a DL model. Deep learning/deep neural networks use complex mathematical modeling techniques to process the data in convoluted ways.

For the development of our deep learning models, the data is collected from several sources such as Google Street View (GSV) and Building Instance Classification (BIC) images. The GSV images are captured using two APIs: Directions Service API and Street View Static API. We have utilized the Directions Service API to collect the geolocations from a source location to the destination and from the given geolocation, the Static Street View API returns the GSV image. Using online house APIs, we downloaded the house dataset from a provided zip code. We have used GSV and BIC sources for houses, greenery, age, and apartment detection.

Data is annotated with Supervisely, a leading platform with a friendly interface and multiple user-elevated tools for a more data-concentrated annotation process. However, using our supervisely to pascal VOC script, we converted the dataset format from JSON to VOC 2007 to feed into the network. Training is carried out via the MXNet/Gluon framework, an innovative toolkit with built-in implementations of various state-of-the-art deep learning algorithms in computer vision and object detection from which researchers can utilize directly for training their model. Gluon is an API that can operate smoothly with Apache MXNet and the Microsoft Cognitive Toolkit (CNTK). We have trained a Faster R-CNN model on Pascal VOC 2007 using this Gluon framework for our object detection models. ResNet-50 is used as a base network model for fine-tuning our Faster R-CNN model. Each model is a stand-alone network that can detect houses, age, and greenery. We adopted different units as we are mainly concerned about the inference latency in the Cloud environment (in milliseconds) and the real-time stream inference speeds (in frames per second) in the edge.

E. Deep Learning Models

House Type Model: The house and apartment deep learning models are trained and validated on 500 and 100 images, respectively. The dataset that is fed to the network is annotated using the Supervisely tool and then converted to Pascal VOC 2007 using a conversion engine. We have implemented these models through the Gluon/MXNet framework, which has built-in APIs that can preprocess the dataset corresponding to the pre-trained model and load the base network model, such as the Resnet-50 from the model zoo. The model is validated with the real-time images that are streamed from the GSV Static API and the images from Google. The real-time inferences result in the location of the object with a confidence score. The result is then plotted and displayed via the Viz module in gluoncv.utils and the popular Matplotlib package.

House Age Model: This model with 3 classes (house ages of 0-20, 21 - 40, 41 - 60) is trained via the TensorFlow framework for classifying the age of houses based on the images. A total of 540 images were collected from online sources and the dataset was split into 450 images for training and 90 images for validation. The model was trained with Inception V3 with the ImageNet pretrained model for 10 epochs with batch size of 8. The average accuracy of the validation was 73%.

Greenery Model: The greenery model is capable of locating environmentally healthy objects such as trees and lawns in series of images of a neighborhood and computing the greenery level based on the detected area of trees via the coordinates of the predicted bounding boxes. A higher score indicates a higher chance of an eco-friendly community. Training and test datasets are collected directly from the GSV API for flexibility, convenience, and ease of mass collection. The greenery model is trained and tested upon a collection of 500 images from GSV with a validation split ratio 80-20. There are two classes: tree and lawn, which are clear signs of greenness in the neighborhood. The tree class consists of trees, bushes, shrubs, flowers, and other similar plants with considerable heights, while lawn represents flatter surfaces such as trimmed grasses and yards. Assuming the training and test images and their corresponding annotation files are structured correctly based on the 2007 Pascal VOC, the greenery model is trained with the Faster R-CNN network on a local machine under the GPU mode with a batch size of 1 and 25 epochs.