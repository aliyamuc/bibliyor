OSμS: An Open-Source Microservice Prototyping Platform

Abstract: One major advantage of microservice cloud architectures is the agility with which microservices can be replicated to help improve the overall quality of service and meet service-level contracts. Their challenge is to carefully balance the horizontal microservice replicas with the vertical resources of CPU, memory, and IO that are allocated to each microservice. The objective of such balancing act is, of course, to avoid both service bottlenecks and resource wastage. In this paper, we present OSμS, a new open-source microservice prototyping platform that has been developed and instrumented from the ground up with the objective of collecting fine-grained, non-proprietary metrology on microservice mesh performance. We will illustrate the use of OSμS for developing and evaluating machine-learning algorithms for the horizontal and vertical autoscaling of microservice architectures. A hybrid algorithm based on decision-tree learning will be implemented on OSμS and compared with the academic state of the art and existing cloud-provider solutions. The advantages of such algorithm in improving horizontal and vertical resource utilization will be highlighted.

Index Terms: Microservices, Resource Allocation, Containers

I. INTRODUCTION

In cloud computing, a microservice designates an independently deployable service built around a specific business function. It encapsulates business capabilities and data but makes them accessible via well-defined network interfaces. Microservices communicate with each other using network links, thus forming fabrics of distributed computing systems. An example of a microservice mesh is that of an e-commerce service comprising ordering, payment processing, invoicing, stock management, shipping, and after-sale service as microservices.

One useful software analogy of a microservice is that of a class in object-oriented programming where information hiding and data encapsulation facilitate the development of large-scale software systems that are easy to integrate and maintain. A more appropriate analogy is perhaps to liken a microservice to a form of business IP (Intellectual Property) whose integration into a full-fledged cloud computing service is enabled by the internal consistency of its own data and procedures and by its well-defined interfaces and IO endpoints. This business IP perspective on microservices is further supported by the fact that a microservice has two views: a logical view and a physical view, and it is often the case that the performance of a microservice mesh is very much dependent on its physical rather than its logic view.

Given the advantages offered by microservices, the cloud industry has been steadily moving from its reliance on rigidly monolithic service architecture to a more flexible distributed architecture. Unfortunately, this trend has not yet been captured in the important context of cloud-native frameworks for scalable, secure, multi-tenant AI of Things (AIoT). Indeed, microservice architectures could provide interesting pathways for addressing the various challenges of AIoT, including multi-tenant resource management, benchmarking and auto-tuning, capacity autoscaling, and accelerator abstraction.

One major challenge to exploring such microservice architectures for AIoT is the absence of a vendor-independent platform for microservice prototyping that can provide fine-grained, non-proprietary metrology of microservice mesh behavior along with real-time Quality of Service (QoS) metrics.

It is therefore the main goal of this paper to present such a platform and to show, at a reproducible level of technical detail, how its elements can be built entirely of open-source software components and how it can be instrumented, also with open-source monitors, to provide a full dashboard view of microservice mesh QoS under a variety of service workloads. Another important goal of this paper is to sensitize the community to the business IP perspective of microservice computing - a perspective that we believe is rich with software design automation challenges that can be addressed in line with the suggested methodology of this article.

Our major contributions are therefore as follows:

We expose and explain the design methodology of open-source microservice architectures.
We give a concrete implementation of the microservice design methodology in the form of a full microservice prototyping platform, called OSμS, using solely open-source software components.
We provide fine-grained telemetry of OSμS to enable microservice mesh performance analysis and illustrate its usage to evaluate microservice throughput and reliability.
We evaluate the proposed autoscaling algorithms on a variety of service workloads and highlight their advantages in improving resource utilization in CPU and memory.
Finally, we illustrate the use of OSμS to prototype and validate algorithms and policies for microservice resource autoscaling.
II. OSμS METHODOLOGY AND ARCHITECTURE

In line with standard software engineering practice, we start our development of the microservice prototyping platform by making explicit the methodology of microservice architecture and design. The principles of microservice architecture are as follows:

Cohesiveness: The microservice should be cohesive, i.e., implementing a small set of related functions.
Compactness: The microservice should be compact enough for easy testing and maintenance.
Isolation: Any new or updated specifications must impact only a single microservice.
Responsiveness: Any microservice should be highly available with minimum latency.
Following the above principles, one can decompose a given service into its microservice components, where each component corresponds to one business function. Also in line with software engineering practice, we follow a top-down methodology in designing a microservice mesh. The methodology flow consists of the following steps:

Unified Modeling Language (UML) diagram: The diagram shows the system microservices and their interactions. It is used to determine the critical path that controls the latency of the microservice.
Microservice instantiation: This is the step that enables microservice replication and, therefore, architectural scalability. Scalability with respect to data and job requests under response time and resource utilization constraints defines microservice performance.
Database selection and management: The database microservice is typically shared amongst multiple microservices. It is also on the critical path of job request handling. It is therefore crucial that the selected database microservice support a variety of data types (e.g., time series vs. images) and a variety of storage and sharing formats.
Load management: In this step, service concurrency is examined, with request latency and throughput being the major concerns. Of utmost importance is the selection of an appropriate load balancer to distribute the load to multiple instances of the same microservice so as to minimize the response time and maximize throughput. It is common to execute time-consuming jobs in the background so as to improve availability.
Monitoring: For actionable microservice monitoring, the microservice evaluation metrics must be crisply defined and the orchestration environment properly instrumented to track and log them. Typically, a dashboard tool is used to visualize these metrics. Each microservice must be thoroughly tested to ensure it meets the specs of its own evaluation metrics.
The UML diagram further shows how a job request propagates through the four primary microservices (web, scheduling, worker, and database). It also reveals microservice dependencies. In case the scheduler or the database experiences a Quality of Service (QoS) violation, then the whole microservice mesh will experience the same violation. In cloud computing, the cascading of QoS violations is called the back-pressure effect, and the cascading path is considered a critical path in the microservice mesh. Fixing QoS violations on such a path is costly in terms of time and resources as microservice dependency increases the complexity of the cluster manager. The manager cannot allocate resources to a given microservice on the critical path without assessing dependency impact on end-to-end QoS. For a given QoS violation, the recovery time of a microservice mesh is typically longer than that of an equivalent monolithic service. In the monolithic case, full recovery is achieved as soon as the cluster manager creates a new instance of the monolithic service and re-balances its workload. On the other hand, in the microservice case, the autoscaler will allocate more resources to saturated microservices upstream from the QoS violation node, even though they are not the culprits of the violation. Several passes through resource allocation and rebalancing are required before the culprit microservice is identified and its resources are increased. By the time the culprit is identified, the workload will have increased, and thus even more time will be needed to return to a steady state. This QoS violation scenario highlights the central role of resource efficiency models in enhancing performance predictability toward the implementation of microservice management policies that reduce latency and improve resource efficiency. In a production environment, the microservice dependency graph typically contains hundreds of microservices, and so the challenges of resource efficiency modeling and model-based resource allocation are daunting. One goal of this paper is to illustrate how a microservice prototyping platform such as OSμS can help addressing these challenges.

The implementation details for each microservice in the UML diagram and the software used are given in Section IV. It is important to note that all the tools we have used in the implementation are open-source.

III. OPEN-SOURCE SOFTWARE TOOLS

In this section, we highlight the various open-source software tools used in the OSμS platform and briefly explain their roles.

A. Microservice Architecture (MSA) Tools

As already highlighted, MSA is a cloud-native architectural style in which a service is built as a set of loosely coupled components of fine granularity that communicate with each other via thin Application Programming Interfaces (APIs). MSA allows more agile development, shorter turnaround, independent component updates, and easier service scale-up.

Containers are the software building blocks of an MSA. Each container bundles its own software, libraries, and configuration files but shares with other containers the services of a single operating system kernel. Consequently, a container requires fewer resources than a virtual machine. Users can run various applications on distinct containers on the same host. Such containers can freely move between host machines, so developers can build, manage, and secure their applications without worrying about the host and network infrastructure.

Docker: Docker is the leading open-source containerization technology in the cloud computing industry. It is a high-level, user-space Linux utility that enables the creation of containers, their execution, and their mobility across hosts. Unlike other container solutions, Docker’s provide a virtual network on top of the host machine, thus allowing seamless and secure intercontainer communication. Docker containers are lightweight, start quickly, and can bundle dependencies and variations of implementations within themselves.

Docker-compose: It is a tool offered by Docker Inc. to run multi-container Docker applications. It allows developers to configure their applications using a YAML file and start them all with a single command. Development, testing, and staging can all be performed from Compose.

B. Monitoring Tools

cAdvisor: cAdvisor (Container Advisor) is an open-source tool that provides the resource utilization and performance characteristics of each container. It runs as a daemon that gathers, aggregates, processes, and logs data pertaining to container runs. However, it only displays real-time data and does not store them. It is natively compatible with the Docker container, but it should work with just about any other container.
Influxdb: Influxdb is an open-source time series database. It is developed in the Go programming language and is used to store and retrieve time series data in a variety of domains, including operations monitoring, application metrics, Internet of Things sensor data, and real-time analytics. It can be used to store the metrics collected by cAdvisor.
Grafana: The Grafana dashboard is an open-source analytical and visualization tool that provides a comprehensive observation stack for monitoring and analyzing metrics, logs, and traces. It enables the user to query, display, comprehend, and alert on data. It has an interactive query builder that the user can use to design complicated monitoring dashboards. Grafana is frequently used in conjunction with time series databases like Influxdb.
C. Programming Frameworks: Python Flask

Flask is a Web Server Application Interface that is simple to use and quick to get started. It has the flexibility to scale up to complex applications. Initially, it was a basic wrapper for Werkzeug and Jinja and has since become one of the most popular Python web application frameworks. Flask makes recommendations, but does not impose any dependencies or project layout. The developer has complete control over which tools and libraries to use. The community has created a number of extensions that make it simple to add new features.

D. Machine Learning Tools: Scikit-Learn

Scikit-learn is a Python package that integrates a variety of cutting-edge machine learning techniques for supervised and unsupervised problems. This package aims to make machine learning accessible to non-specialists by utilizing a general-purpose high-level language (Python). The package is easy to use with minimal dependency. It depends only on numpy and scipy and is under a BSD license, making it suitable for usage in both academic and commercial environments.

IV. OSμS IMPLEMENTATION AND TESTING

A. Implementation

Multiple Docker containers are used to manage component interaction, run the user code, schedule resources, and monitor devices. Users access the system microservice through a web browser that is also used to upload their jobs and data and check the status of their program execution. A flexible management layer for efficient resource sharing and management can be provided in the cloud with these features.

The Microservice Framework: Seven Docker microservices are created according to the following division of labor:

Web App: This container is a Flask web application server that provides access to the system services via a web browser. It submits job requests to the job queue and stores user data. It also returns the status of the submitted request and the results once the job is completed.
Nginx: This container runs the Nginx server that serves as a load balancer for our web app. The main advantage of using Nginx is making our web application faster, maximizing our system performance, and reducing server downtime. Nginx can scale our system performance according to traffic requirements. In our implementation, Nginx is configured to use Round Robin as the load balancing algorithm, and all web servers are assigned the same weight.
Postgres: This container runs the PostgreSQL server that is used to access the PostgreSQL database. PostgreSQL is an object-relational database management system (ORDBMS) that can handle workloads ranging from small applications running on a single machine to large web-facing applications with many simultaneous users.
Data: It is a separate volume container based on the PostgreSQL image that is used to store the user’s application and data. This dedicated volume is used to protect data in the event of a Postgres container malfunction.
Redis: A Redis server is running inside this container. Redis is an in-memory queue data structure used to submit user requests. Redis is very fast and lightweight and can be scaled to send up to millions of messages per second. In our implementation, the web app and worker microservices have asynchronous communication to avoid keeping the web server busy waiting for the worker to finish the long-running tasks. Therefore, we have used Redis as a task queue, to which requests are passed with the worker container running the long tasks in the background. This enables the sending of immediate responses back to the clients. All submitted requests have the same level of priority and are scheduled based on the FCFS scheduling algorithm.
Worker: It is a background container responsible for executing and running the submitted job on the underlying infrastructure. The container has access to the user data and code, so it manages the execution of the code and reading/updating the data. In our implementation, we have used RQ (Redis Queue), a Python library backed by Redis. It is used to queue the jobs in the Redis queue and process them in the background with workers.
RQ dashboard: It is a lightweight Flask web front-end used to monitor the status of the enqueued jobs and the status of the workers performing the background tasks.
The interactions between the various components of the proposed system are shown in the sequence diagram. Some of these interactions occur within the Docker containers, while others occur between an external host and a container. The web browsers upload their jobs and data files through the web container, which in turn stores the data in the Postgres database. The Web App will also add a job into the Redis queue, which triggers the worker container to perform the steps needed to execute and run the job in a container. If there is no error, the job runs on its allocated HW resources, subject to availability. Otherwise, the user request will remain in pending status until the minimum requirements of the hardware resources are available. The generated executable file is stored in the database for future user requests to execute the program. Finally, the service administrator uses the RQ dashboard to check the status of the different jobs and keep track of the failed jobs.

For efficient resource management and for monitoring the submitted jobs, we add the following metrology components to the framework:

Microservice monitoring is the process of collecting basic metrics like memory usage, CPU usage, file system usage, and network I/O in real-time for the microservice replicas. Microservice monitoring is crucial to detect performance problems, avoid system outages, and quickly resolve issues to help applications run smoothly. We have used three main tools: cAdvisor (collect real-time metrics of running containers), Influxdb (store metrics collected by cAdvisor), and Grafana dashboard. The latter tool is used to visualize the data collected in Influxdb by running queries against the database and plotting them accordingly.
Job-level monitoring is the process of monitoring the status of jobs submitted to the framework and performing debugging on failed jobs. It also reports the status of the workers and provides statistics on the workers’ performance. This is achieved using the RQ dashboard. The dashboard also provides information about the system queues, such as how many jobs are in the queue.
In OSμS, we focus on the following QoS metrics to monitor the system microservices and improve its performance: (1) Throughput: This is the number of tasks that were completed successfully per unit of time (i.e., the success rate). (2) Cost: The cost model is the utilization and consumption of hardware resources. The goal is to build an efficiency model that manages resource allocation to the different microservices, thereby reducing overall operating costs. (3) Latency: Our objective is to reduce the average waiting time of the jobs in the queues as well as the response time of the web services used for job submissions. (4) Availability: The main goal is to ensure the continuous availability of all microservices and to implement automatic recovery mechanisms post failures. Another goal is to scale the microservices and balance the load between the different instances of a microservice, especially at peak time.

B. Testing and Evaluation

The OSμS platform is designed with the ability to expand services efficiently and ensure that all services work together without resource contention. To show the importance of the management layer, we have performed various experiments to study the behavior of OSμS under different loads, locate the critical path, and explore the exacerbated tail-at-scale effect.

Experimental Setup: We have performed our experiments on a Dell workstation with the following specifications: (1) CPU: Intel Xeon(R) CPU X5680 3.33GHz x 24; (2) Memory: 23.5 GiB; (3) OS: 64 bits, Ubuntu 18.04.3 LTS; and (4) Storage: 452.7 GB. All services are running on a single host. Docker-compose is used to configure the services and create a network for inter-container communication.

Performance Testing: This test measures the responsiveness of the microservice mesh under a specific workload. We will focus on two metrics: response time (the time the mesh takes to respond) and load factor (the number of users sending requests simultaneously). We have performed the testing in three different cases. For each case, we performed three different load tests (test 1: load = 5; test 2: load = 10; and test 3: load = 15). For each test, we calculate the system response time, the number of requests per second, and the number of failed requests. As expected, the average response time of the system decreases as more web servers are recruited.

Redis Queue and Workers Performance: To evaluate the worker microservice, we carry out functional and non-functional testing on its replicas. For the non-functional testing, the goal is to find the worker’s average working time, the relation between the number of workers and system throughput (number of finished jobs per unit time), and the average wait time for a job in the queue.

For functional testing, we are targeting CPU-intensive workloads. We implemented the same function to simulate intensive workloads. The proposed process generates random strings, each 1000 characters long. The number of generated strings is a random multiple of 1000 between 1000 and 10000. The generated workload is as authentic as real-world apps. For the proposed simulated workload, we have implemented a Service Level Agreement (SLA) to avoid very long tasks, namely, the execution time for the CPU-intensive function should not exceed 20 minutes. Otherwise, the job will be labeled a failed job (i.e., an SLA violation has occurred).

We have performed three different tests, each with a different number of workers (5, 10, and 15 workers). In each test, we have submitted around 100 requests to the queue. Clearly, the number of failed jobs decreases as the number of workers increases.

Docker Monitoring: While performing the previous tests, we have collected resource usage metrics for the web container(s), Nginx, worker container(s), and Redis in all different cases. For load testing, we have found that the memory usage, CPU usage, and network I/O per web container decrease as the number of containers increases. Additionally, all web containers have almost the same usage metrics. This shows that resource usage is equally distributed amongst the containers. The file system remains the same in all cases.

Discussion: In all tests performed, none of the containers overused any system resources or caused any problems for the application while running. Furthermore, the monitoring tools were themselves running as microservices on the same host. This demonstrates that the OSμS implementation is lightweight, robust and has almost zero downtime, as it did not encounter any problems or suffer any crashes during the experiments. Interestingly, although several aspects of the OSμS architecture are asynchronous, no deadlocks have been reported, all services worked together smoothly, and there was no over-utilization of resources. The worker experiments illustrate the need for microservice autoscaling to reduce SLA violation rate and maximize system throughput. On the other hand, the load testing results show that adding more replicas does not mitigate the failure rate of the web microservice. The debugging shows that the database microservice may become overwhelmed with an excessive number of concurrent requests if it is not configured properly. When the database failure propagates from the back-end microservice to the front-end microservice, scaling up the front-end (or web) microservice alone will not improve the system’s performance. This is an example of the exacerbated tail-at-scale effect discussed in Section II. In such a case, the problem is implementation-dependent and should be addressed by the system developer. Here, the metrology components of OSμS become crucial for the development of an appropriate solution.

All in all, the experiments emphasize the need for an efficiency model that can allocate the hardware resources needed and find the optimal number of replicas for microservices to minimize the SLA violation of the system and the execution cost, and maximize the resource utilization.

V. OSμ4S EXAMPLE: HYBRID AUTOSCALING

In this section, we give a concrete example of using OSμS to develop a microservice management layer consisting of a random forest model that can predict the amount of resources and service replicas for maximum resource utilization and minimum QoS violation. This management layer will be architected as an autoscaler according to the "MAPE" paradigm of a control loop model (Monitor, Analyze, Plan, and Execute).

In particular, the analyzer is a Random Forest (RF) regressor of decision trees trained to produce resource predictions with small amounts of training data. The model has been trained with the fastStorage dataset, which consists of very dynamic workload traces of different software applications running on 1,250 VMs. The trained RF model has a very high R2 score (∼ 0.8) and a low RMSE (∼ 2).

As for the planner, a hybrid autoscaler has been designed for efficient resource utilization and high end-to-end performance. It comprises a vertical and a horizontal autoscaler that are combined in a single resource allocation policy.

We have used the OSμ platform to evaluate the hybrid autoscaler using a variety of different loads to simulate the usual conditions seen by most data centers. We simulate peak and off-peak "hours" of client traffic to examine how the algorithm reacts to steady and fluctuating loads. The total number of submitted tasks by the end of the load test is 900. Throughout the experiment, we did not face any failures in any of the microservices, and none of the submitted tasks failed. All tasks were successfully completed before their due date. Throughout the load test, we monitor and collect the resource usage of all running containers and use them for scaling decisions.

For horizontal scaling, throughout the different loads, we found that 1 CPU core was enough for all containers and all microservices except 'Worker', and there was no need to add replicas to meet the high number of requests. It is important to note that scaling the workers is independent from the request rate to the web microservice, as the scale depends solely on the predicted utilization values.

VI. CONCLUSIONS

In this paper, we have presented OSμS, a new open-source microservice prototyping platform that has been developed and instrumented from the ground up with the objective of collecting fine-grained, non-proprietary metrology on microservice mesh performance. We have illustrated the use of OSμS for the development and evaluation of machine learning algorithms for the horizontal and vertical autoscaling of microservice architectures. The hybrid algorithm, which is based on random-forest learning, has been implemented on OSμS and compared with the academic state of the art and existing cloud-provider solutions. The advantages of such an algorithm in improving horizontal and vertical resource utilization have been highlighted.

Future work on this platform includes its extension to include hardware acceleration microservices in line with the acceleration abstraction challenge of AIoT. A good candidate for such an extension is FPGA as a microservice. Another promising OSμS extension is its use to explore microservice resiliency in line with the fine-grained replication paradigm. The use of OSμS to prototype novel resource allocation policies can, of course, be greatly expanded to include policies based on formal optimization methods. Finally, given the full visibility provided by OSμS, it can be used to explore distributed microservice architectures for securing AI platforms.