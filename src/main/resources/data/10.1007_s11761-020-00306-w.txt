Abstract
Supervised deep learning has achieved remarkable success in various applications. Successful machine learning applications, however, depend on the availability of sufficiently large amounts of data. In the absence of data from the target domain, representative data collection from multiple sources is often needed. However, a model trained on existing multi-source data might generalize poorly on the unseen target domain. This problem is referred to as domain shift. In this paper, we explore the suitability of multi-source training data selection to tackle the domain shift challenge in the context of domain generalization. We also propose a microservice-oriented methodology for supporting this solution. We perform our experimental study on the use case of building energy consumption prediction. Experimental results suggest that minimal building description is capable of improving cross-building generalization performances when used to select energy consumption data.

Keywords: Data selection, Domain generalization, Knowledge transfer, Data-driven modeling, Energy consumption modeling

1 Introduction
Predictive modeling in buildings plays an integral part in the efficient planning and operation of power systems. Adequate operational data are usually a prerequisite, especially when deep learning is adopted. Powerful machine learning models should rely on the insightful utilization of relevant operational data in a sufficient amount. Nevertheless, building historical data is not always available, such as in newly built and renovated buildings.

Renovation or replacement of existing buildings considers improving their energy efficiency based on energy-saving measures (e.g., enhanced thermal insulation, highly energy-efficient electrical systems). It plays an important role in reducing the total energy consumption and lowering the greenhouse gas emissions of the existing building stock. Modeling of these buildings thus poses a challenge since we do not have prior knowledge about their improved energy consumption performance.

Already existing energy consumption data about other buildings can, however, be obtained. The main idea of our work thus consists in leveraging representative data from multiple different (but related) source buildings. However, with possible domain shifts among multi-source and target data, it is improper to apply a single model via combining all multi-source data. Domain shift is a key challenge where distributions mismatch across different data domains. Therefore, models trained on one or many source domains generalize poorly when applied to a different target domain. Namely, in our context, the energy consumption profile in buildings depends considerably on several contextual factors, such as the building type (e.g. residential, commercial, office), size, age, location, etc. Combining energy data from disparate source buildings to model a target building, from which no operational data is available, is consequently counterproductive and will adversely hurt the target performance.

Proposed approaches addressing the domain shift challenge are mainly classified into domain adaptation and domain generalization. Domain adaptation utilizes labeled source data and unlabeled or sparsely labeled target data to obtain a well-performing model on the target domain. However, in several cases, the target data is not available. Domain Generalization (DG) addresses such cases by utilizing multiple source domains. This paper considers the domain generalization area of research. We aim to train accurate predictive models that perform well on unseen target buildings, which have no operational data, by leveraging knowledge from different but related source buildings. We also suppose to have a contextual description of the target building that can be utilized for source data selection. Data selection therefore enables the utilization of the most relevant source buildings based on their contextual similarity to the target building to be modeled.

For this purpose, we investigate the suitability of a data selection approach for cross-building domain generalization. To the best of our knowledge, our work is the first attempt to model a target building with minimal contextual information about it, and thus tackling the data unavailability problem by transferring knowledge from auxiliary buildings. Prior studies in this framework require labeled data of the building in question, such as historical consumption data, physical parameters of the building design, meteorological conditions, and/or information about the occupancy profiles, in order to train a reliable building energy consumption model. Our approach goes beyond state-of-the-art methods and proposes to transfer knowledge across multiple source buildings while using minimal contextual information about the target building. This allows us to model buildings when we do not dispose of energy consumption data, such as in the case of renovated or newly-built buildings. To summarize, our main goal is to build a model that accurately predicts the future energy consumption of a previously unseen building, given training data from one or many selected buildings. For supporting our implementation, we propose a microservice-oriented system workflow that promotes scalability and elasticity when deployed in the cloud.

2 Approaches to domain generalization
Domain generalization is a form of transfer learning, which applies expertise acquired in source domains to improve learning of different but related target domains. Domain generalization focuses on the generalization ability of previously unseen target domains, in which no data is available during training. Proposed domain generalization approaches typically rely on the assumption that source domains and unseen target domains share common features that can be extracted. Hence, they seek to learn a domain-agnostic representation or model. Domain generalization approaches proposed in the literature may be roughly classified into three categories: (1) Data representation-based techniques that seek to learn domain-agnostic representation that captures similarities across domains and where the domain discrepancy is minimized. (2) Ensembling techniques that aim to build ensembles of per-domain models that will then be fused at test time. (3) Meta-learning-based techniques that rely on a model-agnostic training procedure that trains any given model so that it mitigates domain shift between domains.

Muandet et al. propose to learn new domain-invariant feature representations by minimizing the dissimilarity across domains via domain-invariant component analysis and a kernel-based optimization algorithm. Ghifary et al. propose a Multi-Task Auto-Encoder (MTAE) that extends auto-encoders into a model that jointly learns to perform self-domain data reconstruction and between-domain data reconstruction. Xu et al. use learned low-rank exemplar-SVMs, which can be defined as a linear Support Vector Machine (SVM) classifier trained on a single positive training instance and all negative training instances, for both domain adaptation and domain generalization. For domain generalization, the authors propose to either equally fuse all exemplar classifiers or use the exemplar classifiers in the latent domain which the target data more likely belongs to. Given multiple source datasets/domains, Khosla et al. propose an SVM-based approach, in which the learned weight vectors are common to all datasets. Li et al. proposed a low-rank parameterized convolutional neural network model for end-to-end DG learning. Li et al. propose a Meta-Learning Domain Generalization (MLDG) approach. It consists of a model-agnostic training procedure that can improve the domain generality of a base learner. This procedure is based on synthesizing virtual training and virtual testing domains within each mini-batch. The meta-optimization objective consists of minimizing the loss in the training domains while simultaneously improving the loss in the testing domain.

Our work is more related to the model selection techniques. We borrow the per-domain model building idea described by Xu et al. However, we select domains rather than models and combine their respective data to form a representative training set. We assume in our case that we dispose of a minimal description of the target domain that will allow us to define our data selection criteria. Some examples of contextual descriptive features are building typology, area, year of construction, and the number of occupants.

Source domain selection has been proposed in the context of multi-source domain adaptation. This allows selecting good sources that are most relevant to the target domain and avoid negative transfer. In multi-source domain adaptation, authors proposed data-dependent regularizer for domain selection. Other works employed all source domains for adaptation but assigned different weights to different source domains. Weights are generally computed on the basis of some similarity measures between target and source domains. Several domain similarity metrics have been proposed for selection such as Kullback–Leibler divergence, Jensen–Shannon divergence, maximum mean discrepancy, the Wasserstein metric, or the Kolmogorov–Smirnoff statistic. Even within one domain, adaptation performance varies significantly depending on the choice of data samples. Other related work in the direction of data selection includes using reinforcement learning to select data during neural network training.

Both domain adaptation and domain generalization aim to learn an accurate model for the target domain by leveraging labeled data from the source domains. The difference between them is that, for domain adaptation, unlabeled data and even a few labeled data from the target domain are utilized for adaptation. Whereas, for domain generalization, target data is not available. Our work falls within the latter case. We solely dispose of a minimal contextual description (metadata) to capture properties of the target domain for knowledge transfer. Some works have proposed to exploit available metadata about domains/tasks in addition to domain data to guide multi-domain learning and multi-task learning. Metadata in this work consisted of semantic descriptors of domain or task and are combined with feature vectors during training. Rather than combining domain metadata and data, we are utilizing target domain metadata for source data selection. This way, we can address the domain generalization setting in which no target domain data is available during training. We therefore propose in our context to select similar source building data based on the target building metadata and build a predictive model for the target building. The following section gives an in-depth description of our proposed methodology.

3 The proposed system
Our system's main objective is to train an energy predictive model for an unseen target building based solely on its contextual description. In our special case, contextual descriptions concern high-level information about the target building we seek to model, e.g., typology, year of construction, location, etc. The training data of the target building’s predictive model is obtained through an energy consumption data selection workflow. Data selection is performed based on the contextual similarity between the target building and the source buildings. The steps performed by our proposed system at each request are shown in Figure 1.

Our approach consists in training a predictive model for an unseen target building via source data selection. Data selection is based on the similarity between the available source buildings and the unseen target building contextual descriptions. We assume that source buildings' energy data and contextual descriptions are pre-collected and stored, whereas the target building's contextual description is provided by system users. Once similar source buildings are identified, their corresponding energy data is retrieved. Energy data from buildings generally consists of historical energy consumption data along with critical exogenous variables such as weather conditions, holidays, etc. Retrieved source data from multiple sources is then combined to form a training dataset and provided to train a predictive model for the target building.

Our system users are mainly building energy professionals and third-party building management systems which seek to accurately model a building on which operational energy data is not available. An accurate prediction of energy demands at the customer and building level will provide useful information to make decisions on energy generation and purchase. In this study, we attempt to explore the suitability of similar training data selection in the context of building energy consumption modeling.

3.1 System architecture and data specification
We propose to establish a microservices-based architecture (MSA) for cross-building knowledge transfer. Each individual microservice is fully independent, self-contained, and specific to a single task. Unlike monolithic applications, the MSA breaks down the application into a suite of flexible, independently deployable, and loosely coupled modules that are accessible via a lightweight language-agnostic application programming interface (API). APIs are mainly based on asynchronous messaging protocols.

MSA offers several benefits, such as an increase in agility in development and delivery, resilience to failure, reliability in operation, maintainability, separation of concerns, and ease of deployment. Compared to service-oriented architecture (SOA), the core intent of the MSA pattern is to limit a service to a single purpose, enabling it to be fully decoupled and thus much more easily scaled and swapped out. Contrary to MSA, component sharing is one of the core tenets of SOA. SOA, therefore, relies on multiple services to fulfill a business request. Whereas MSA minimizes the need to share components through bounded context, which allows the coupling of a component and its data as a single unit with minimal dependencies.

Our system is capable of continuously ingesting and integrating data from external providers such as weather data and open energy data. Time series data about building energy consumption and weather data are, respectively, stored in the time series store and the weather data store. These two stores are linked together through the contextual information. In addition, contextual information provides a high-level description of the building environment, such as the year of construction, the building type, the size, and the number of occupants.

The entry point of our system workflow is the data selection step. Via our system’s API, users define the required use case by providing a key-value description of the unseen target building to model. No prior knowledge of the target building’s energy consumption is needed. The most relevant time series data corresponding to the most similar buildings is then identified and selected. Similar buildings are identified based on the contextual information on the target building and the contextual information on other source buildings available within the system. The training data selection service loads contextual information from the contextual store via message queues.

Once similar source buildings' identifiers are available, the predictive model learning service will load corresponding data from the time series store and/or weather data store via message queues. The training dataset will then be prepared using data transformation techniques, e.g., missing data imputation, outlier removal, etc. Finally, the predictive learning model is trained in order to predict future energy consumption for a pre-defined forecasting horizon. In the current work, we rely on a recurrent neural network for predictive modeling.

Our training data selection workflow starts at each user request. It parses the contextual information about the target building contained in the request and studies its similarity with pre-stored contextual information about available source buildings. Data in our system is shared between microservices following event-based communication. Microservices, therefore, communicate via event messages. This enables loose coupling between collaborating microservices and privileges asynchronous behavior. Once similar source buildings are successfully identified, their identifiers are shared with the predictive model learning service. Building data and weather data handling microservices play the role of data providers when selecting training data and training predictive models. The building data handler provides contextual information and energy consumption time series data about available source buildings to, respectively, the training data selection microservice and the predictive model learning microservice. The weather data handler provides exogenous weather data, such as air temperature, atmospheric pressure, and wind speed, to the predictive model learning microservice.

To deal with potentially large-scale data, we rely on a multi-modal data store in the backend. Time series data is stored in a traditional relational database management system (RDBMS). Our system is transparent to the specific database technology used. Contextual data about buildings and their associated time series is stored in a graph database.

3.2 Suitability of training data selection
In this study, we investigate the suitability of training data selection for cross-building knowledge transfer. The main logic behind our suitability study consists in training a predictive model using time series data of each building available in the dataset. Then, we test the cross-building generalization performance of each resulting predictive model, i.e., test it on other unseen buildings in the dataset. This will allow us to study the correlation between good generalization results between two buildings and the similarity between their contextual information. We can therefore study the possibility to select representative building time series data based solely on available target building contextual information.

Considering for example the task of energy consumption prediction for a residential building occupied by two people, built in 1990, renovated in 2014, and located in Lyon. Having no operational data about the target task, it is required to utilize other operational data on different source buildings to build a predictive model. However, different data collected from distant source buildings would necessarily induce negative transfer. We thus study a method that will enable us to select only similar buildings that will yield efficient cross-building prediction results. For example, we select residential buildings that are constructed around the same year, located in a region with a similar climate, or subject to a similar occupancy profile as the target building.

In our experimental study, we propose to compute similarities between the target building and source buildings' contextual information using a pairwise distance. Computational complexity of data selection is therefore O(n), where n is the total number of available source buildings. Predictive models then learn to predict future building-level aggregate energy consumption based on energy consumption history and both past and future climate data. In this work, we focus on the meteorological data factor by feeding our model with past and future climate data along with the aggregate past energy consumption. The motivation behind utilizing both future and past climate data is to attempt to capture the correlation between day-to-day weather conditions changes and the building’s energy load profile.

3.3 Predictive model learning
Recently, deep learning is widely adopted for building energy consumption prediction tasks. Various deep learning models have been used, e.g., recurrent neural networks (RNN), sequence to sequence (Seq2Seq) models, combinations of convolutional neural network and recurrent neural network (CNN-RNN). In this work, we propose a unidirectional Long-Short Term Memory Recurrent Neural Network (LSTM-RNN) for the predictive modeling task. RNNs are a powerful class of supervised machine learning models that are capable of modeling sequential data. They are artificial neural networks where connections between units can form cycles, which allows propagation of hidden state information from early parts of the sequence back to a later point. LSTM is an RNN architecture that helps to prevent the effect of vanishing and exploding gradients often encountered in conventional recurrent networks. LSTM offers the ability to pass information selectively across sequence steps while processing sequential data one element at a time.

Our model is trained to predict the daily energy consumption of the subsequent week. As input, we provide the daily energy consumption of the previous week and climate time series of the subsequent week.

Our training set X = {(x (1), y(1)), (x (2), y(2)), . . .} is structured into time-based sequences of fixed length. Input sequences are denoted by (x (1), x (2), . . . , x (T )) where T denotes the sequence length, and each value x (t) ∈ R7 for t ∈ 1 . . . T. Feature vectors are composed of current week’s aggregate energy consumption, air temperature, average horizontal solar irradiance, wind speed, and these same features for the subsequent week. Similarly, target sequences are denoted by (y(1), y(2), . . . , y(T )), where y(t) ∈ R is a vector denoting the energy consumption at future time steps. The goal of the model is to predict future energy consumption y(t) from the input feature vector x (t).

The architecture of the network is composed of several hidden layers. It consists of one or more LSTM layers followed by one or more fully connected layers. The output layer is a fully connected layer with a linear activation function. The model is trained using the Root Mean Squared Error (RMSE). We also use the batch normalization mechanism to address the internal covariate shift problem usually encountered in deep neural networks training. Training phases were conducted using the Backpropagation Through Time (BPTT) optimization algorithm in the context of LSTM networks. BPTT is commonly used to train recurrent networks. It “unfolds” the neural network in time by creating several copies of the recurrent units which can then be treated like a feed-forward network with tied weights. The BPTT algorithm is known to be computationally efficient, having a computational complexity per time step of O(W), where W is the number of weights.

During our experimental study, we explore variants of this architecture to fine-tune its hyperparameters, e.g., number of fully connected layers, number of LSTM layers, etc. We retain the architecture variant that yields the best cross-domain and in-domain generalization results.

4 Experimental setup
We perform our experimental studies on the use case of building energy consumption prediction. Our system transfers knowledge from several buildings to one target building on which we assume we are facing a data unavailability problem.

4.1 Dataset
The proposed solution is experimentally evaluated using the REFIT Electrical Load Measurements dataset. The dataset contains cleaned electrical consumption measurements for 20 UK households at aggregate and appliance levels. For each household, the whole house aggregate loads and nine individual appliance measurements at 8-s intervals were collected continuously over a period of approximately two years. During monitoring, the occupants were conducting their usual routines. In this paper, only the aggregate electrical consumption values for the whole house are used. We work with 1-day resolution data which were obtained by summing the original data.

In addition, climate data was also collected from a nearby weather station. The REFIT dataset description includes the number of occupants, construction year, number of appliances, building type, and size. Buildings' similarities are analyzed based on these descriptions, which are one-hot encoded as a further pre-processing step. The Euclidean distance is used to compute pair-wise similarities, and clustering results are illustrated using a dendrogram.

4.2 Model training
For each building, we use data between April 2014 and May 2015 for training. For cross-building evaluations, we use data between April 22nd, 2014, and June 1st, 2014. The whole dataset was scaled so all values will be between 0 and 1, using the min-max normalization algorithm. The input and the output sequences are of length 7. The input corresponds to a 7-dimensional feature vector. Our network is composed of two hidden layers; one LSTM layer of size 256, and one fully-connected layer of size 128. The Rectified Linear Unit (ReLU) is used as the non-linear activation function for hidden layers. The output layer consists of a fully connected layer with a linear activation function. The fine-tuning of weights is done using the Gradient Descent algorithm with an exponentially decaying learning rate ranging between 10−3 and 10−5. Weights initialization follows a normal distribution with zero mean and standard deviation σ = 1, whereas biases are initialized to zero. The gradients are back-propagated through timestep batches of length 80. For the training epochs, we have fixed 1000 as the maximum number. To avoid over-fitting, we have implemented an early stopping mechanism which breaks the training loop when training cost does not improve on the training set after 20 epochs.

4.3 Experimental results
Our goal is to achieve good generalization performance by accurately predicting the short-term energy consumption of unseen buildings. Therefore, we assess our proposed model using the Root Mean Squared Error (RMSE). RMSE is defined as the square root of the average squared distance between prediction and ground truth.

We trained 19 models for each building following the same process. One building (number 12) was not considered due to insufficient training data. Each model was tested on the remaining unseen buildings in order to study its cross-building transferability. The predictions errors of cross-building model transfers are visually identified in two clusters. Buildings that are judged similar based solely on their descriptions do yield good prediction results when performing cross-building knowledge transfer.

5 Discussion
The most similar buildings based on their descriptions are clustered under the same cluster based on their cross-domain generalization errors. This means that models trained on one building will generalize well when applied to similar buildings. Poor cross-domain generalization performances of certain buildings are explainable by their dissimilarity with the rest of the buildings.

In this study, we leveraged a very restricted set of building descriptions. Therefore, we believe that more heterogeneous and broader building descriptions would help to select similar data more accurately and more reliably, making results more consistent. Data selection approaches based on similarity metrics are essential in order to perform large-scale and accurate cross-domain domain generalization.

6 Conclusion and perspectives
This paper discusses the suitability of the data selection approach for cross-building knowledge transfer. Evaluation work was conducted on the case study of building energy consumption modeling. For this purpose, we have trained per-building models and studied their transferability across other unseen buildings. Experimental results show that minimal building descriptions are capable of guiding domain generalization applications in the context of energy modeling, by identifying similar buildings. Our results confirm the suitability of data selection mechanisms that are based on similarities of building minimal descriptions.

We also propose a microservice-oriented architecture that offers increased evolvability and scalability of the system as well as accelerated development velocity.

Future work involves exploring and reporting the behavior of our approach with larger scale and higher heterogeneity datasets. We also intend to extend our system by automating the data selection algorithm based on user queries. User queries will contain the description of the target building to which we want to transfer knowledge.