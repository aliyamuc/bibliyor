THURSDAY: A Web Platform to Support AutoML
Chase Carthen, Christopher Lewis, Vinh Le, Alireza Tavakkoli, Frederick C. Harris Jr., and Sergiu Dascalu
University of Nevada, Reno
Reno, USA

Abstract
THURSDAY is a web platform that aids users in building machine learning models by providing easily accessible tools to either create models manually, or through the use of automated machine learning (AutoML) libraries like AutoKeras. As part of THURSDAY’s key innovations, users are given the opportunity to configure and run multiple machine learning models. The results of these model executions can then be compared with built-in performance metrics. Finally, THURSDAY allows users to analyze hyperparameter changes, as well as the changes created by AutoML libraries, in order to provide a vital tool that aids in the revision of existing models. To meet the high volume demands of machine learning, THURDAY adopted a microservice-based design pattern that supports containerization, orchestration, and scalability. In this paper, the design, implementation, and impact of the THURSDAY system is explored in detail. In order to evaluate the capability of THURSDAY, its core functionality is compared against similar platforms that provide machine learning support.

Keywords
automl, visualization, deep learning, human computer interaction, neural networks

1. Introduction
The inclusion of machine learning (ML) in research projects often presents itself as a steep risk for new researchers, due to the sheer amount of time it requires to learn and configure a workflow. Finding the optimal ML model and the parameters associated with that model can steal away a significant amount of time, even for professionals and those savvy with ML. Researchers employing ML in their workflow may soon find that they spend more time on learning ML and contending with parameter tuning than the actual research of their domain.

Interestingly, many researchers have identified this gap between learning/configuring ML and utilizing it in their domain research. It is through this need that the sub-field of Automated Machine Learning (AutoML) emerged. AutoML’s express purpose is to bridge this research gap by developing tools to automate the process of searching for optimal models and identify why the tuning of certain parameters produce more effective results. THURSDAY is designed with this purpose in mind and operates as a means to integrate AutoML tools into a usable platform with powerful tools and services to cater to researchers seeking a potential ML component in their domain research.

THURSDAY itself is a platform consisting of several powerful ML tools designed to ease the burden of researchers. One such tool is a customized visualization designed with AutoML in mind. This visualization provides a visual breakdown of differences between ML models generated from AutoML libraries, but although it supports more than one type of model, it is aimed primarily at deep learning models. The visualization presents differences between models, and these differences include metrics such as the loss, accuracy, precision, recall, computation time, and memory usage. Along with comparing models generated from AutoML libraries, users are able to create a machine learning model themselves and compare it against other models. This functionality is included so that the user may explore any form of model that may be of interest. These functionalities require that THURSDAY’s software be robust enough that it can scale to accommodate multiple users.

In execution, THURSDAY is a full-stack application designed so that it can be placed on clustering infrastructures, like Kubernetes. At the time of this deployment, THURSDAY is deployed and configured with Kubernetes in mind. THURSDAY’s various subsystems and components are all designed and implemented with Docker containers as its primary source of containerization. This is done so that a certain degree of scalability can be ensured as development continues. To allow THURSDAY to be used by almost anyone, this scalability is core to the implementation of THURSDAY, as not every user has a powerful enough machine to develop using AutoML or ML libraries. A database is used to keep track of previous models and datasets used by THURSDAY. This paper covers only the use case of THURSDAY with one AutoML library, AutoKeras. The design and implementation of this software are further discussed later in the paper.

The rest of the paper is structured as follows: Section 2 describes a general background of AutoML and the problems that THURSDAY is being designed to solve; Section 3 describes similar work related to THURSDAY and this problem domain; Section 4 covers how THURSDAY is designed and implemented, along with some limitations; Section 5 does a comparison against existing AutoML tools and other behavior when implemented; Section 6 describes the core functionality and unique aspects of THURSDAY; and lastly Section 7 outlines the future directions of THURSDAY and the overall contributions it brings.

2. Background
Within the AutoML field regarding deep learning, researchers have been working on improving a number of topics. A few topics that we want to highlight are: hyperparameter tuning, meta-learning, neural architecture search, and explainable autoML. Hyperparameter tuning refers to searching for the best set of parameters that would lead to the best machine learning model. An example of a hyperparameter would be the bias of a neuron in an artificial neural network (ANN), or the number of folds to split a dataset into. Typically, hyperparameters are static in nature and must be specified by the developer of the model.

The goal behind meta-learning is for a machine learning model to retain what it has learned from one problem. Then, when the machine learning model is trained for another problem, the retained information from the past plays an active part of the new solution. In more layman terms, meta-learning’s goal is for a machine learning model to be able to apply past knowledge to new problems, and continuing this process again and again onto future problem(s).

Neural architecture search (NAS) consists of searching for the best neural network that will solve a problem in an automated fashion. Methods of NAS can be broken down into three different properties: search space, search strategy, and performance estimation strategy. Search space encompasses looking for the best architecture for a given model that may have constraints such as the number of neural network layers or number of neurons in a neural network. Search strategy is how the search space is explored such as random search, breadth-first search, and other types of searchers. Performance estimation strategy is how to explore the estimation of a neural network.

Explainable AutoML (xAutoML) aims to make AutoML more explainable and improve its transparency. Major talking points within xAutoML include subjects like hyperparameter importance, automatic ablation studies, visualizing hyperparameter effects and sampling process. Hyperparameter importance refers to finding which hyperparameters are important globally. Automatic ablation studies refer to finding what configuration of an AutoML tool was important after changes. Visualizing hyperparameter effects and sampling process refers to visualizing the results caused by altering the hyperparameters and the sampling process. In THURSDAY’s case, this system would categorize as an implementation of the xAutoML methodology, as its main purpose is to assist a user in interpreting the changes made by AutoML libraries.

Typically, the pipeline of AutoML can be broken down as: data preparation, feature engineering, model generation, and model evaluation. The steps for the pipeline of AutoML are shown visually in Figure 1. Data preparation characterizes how data has to be changed, details added, or even corrected in order for a machine learning model to use the data. In order for a machine learning model to learn a specific solution, it often requires some feature within a dataset that can be designed by the user or discovered by the model in the case of deep learning. This process is known as feature engineering. In order for a task within machine learning to be achieved, a model must be built. Model generation is often done by hand and the model is chosen based on the type of problem that is presented. The goal of AutoML is to automate this task of building the model’s architecture in model generation. Training these models can take time and would be inefficient if evaluated this way. There are techniques for estimating how well the model performs without fully training the model and this process is known as model evaluation. The combination of model generation and model evaluation encompasses NAS.

In order to know if an AutoML framework or library is producing an effective machine learning model, a set of metrics are needed to validate whether a model is better performing than another model. These metrics are dependent on the type of output the model has, which could be classification or regression. These metrics can include mean squared error, training loss, accuracy, precision, and recall. In practice, metrics for validation determine whether the changes to the data or machine learning models produce better performance in an AutoML library. THURSDAY visualizes these types of metrics dynamically based on the AutoML library that it interfaces with and what metrics that library displays natively. The visualization of these metrics, with a representation of the machine learning models, can help inform a user of what produces a better result for a given problem.

4. Implementation
4.1 Requirements and Use Cases
THURSDAY’s requirements were generated from use cases. Requirements and use cases were all created based on two main factors: usability and visualization. Requirements and use cases involving usability allow the user to do some action inside of the system. Requirements and use cases involving visualization allow the user to see data THURSDAY generates, or actions that THURSDAY is currently executing.

THURSDAY had the usability requirements that allowed the user to: load tensorflow and labeled datasets; limit the number of layers and size of the models; allow the model parameters to be configurable; allow the user to set the number of epochs, validation techniques, and number of layers; allow an in-training model to be stopped or started; save and load a model, and the code associated with it; specify a time and epoch limit for run duration; allow an AutoML library to be used and configured; and store, display, and evaluate past models.

THURSDAY had the visual requirements that allowed the user to visualize: different colored graphs for each model; a model’s epoch, accuracy, and what validation techniques were used on it; a model’s running accuracy and error rate; the current status of the model (running, running duration, finished, and error status messages); and previous models and their evaluations.

4.2 Back End
THURSDAY’s back end currently uses AutoKeras as the AutoML framework that is being used to compare ML models. THURSDAY uses a PostgreSQL database to store both the ML model being worked with and results from training a model. Flask is implemented as a connection between the front end, the database, and the AutoML framework. Each object in the back end is also containerized using Docker. This includes the AutoML framework functions, the PostgreSQL database, and the Flask code that opens up endpoints for a web interface to access. These containers are then orchestrated with Docker-Compose, though Kubernetes is being considered as a replacement when doing future work for THURSDAY. Using this system, THURSDAY is able to expose the back end to the front end in a controlled way that also allows further scalability in the future. Another result of containerizing THURSDAY is that, once hosted, it allows multiple users to use THURSDAY independently from one another. Once this hosting takes place, users won’t need to use their own machine for machine learning.

4.3 Front End
THURSDAY’s front end uses Angular to connect to the back end’s exposed endpoints, created in Flask. The UI is created using Angular, Bootstrap, and Plotly.js. Figure 3 shows the evaluation tab from the front end UI. This figure also shows a graph that is made in Plotly.js off of selected results from the back end. A more detailed example can be seen in Figure 4. Figure 3 also shows bootstrap in the form of navigation tabs at the top bar of the screen, dropdown menus for data selection, and a button for refreshing the graph. The front end is also containerized using Docker. When all of the containers are orchestrated together using Docker-Compose or Kubernetes, this is called a pod. This pod represents one total application for one single user. Any of these containers in a pod can be swapped out for either updated containers, or to be replaced entirely with whatever the user might want at a time. For example, if we found a better replacement for our PostgreSQL database, we could create a similar container to the PostgreSQL database container and then have an automated script pull all of the data out of each PostgreSQL database for each pod. This data would then need to be put into the new container so that users don’t lose data, but the actual swap between the databases would be almost instant after the data was moved in each individual pod.

5. Evaluation
5.1 Features Implemented
Core functionality for THURSDAY has already been implemented. These features include:

The front end and back end THURSDAY are implemented and connected together.
The front end can display data from the database for previous model runs such as loss, accuracy, precision, recall for both training and validation datasets.
The back end stores machine learning models and configurations sent down by the user.
The front end can start and display AutoML jobs on a select set of datasets from TFDS.
AutoKeras has been integrated with the back end.
We have dockerized and orchestrated all services for this project into Docker-Compose.
Essentially, we have implemented sufficient functionality for the whole workflow of THURSDAY to work. For example, a user can pick the MNIST dataset with any number of parameters as shown in Figure 5. See the results within some time like those shown in Figure 3. AutoKeras was chosen as it seems to be the easiest library to work with given the documentation made available for it. The datasets provided by TFDS were chosen because Google provides easy access to the datasets, and the API to interface with them is intuitive. The readily available documentation for Angular and Bootstrap made it possible for us to build a good and functional user interface. Flask made it quick and efficient to set up endpoints to be observed by the front end. Plotly allowed for model runs to be easily visualized within a web browser and certainly made it easy to include it in this paper in Figure 4.

With these implemented functionalities, more functions can be implemented with ease. These features include: being able to visualize models stored in the database, using existing models to evaluate against other datasets, allowing users to create custom models, and the ability to upload custom datasets. Adding these new functionalities will allow expanding on different areas that can be explored. For example, if a researcher finds that a particular model generated by an AutoML library produces an interesting result, they can try creating that model in the custom interface and tweak some aspect of it. The plans for the future work are discussed further in Section 7.

5.2 Software Comparison
THURSDAY, as specified in Section 3, is comparable to CAVE, Katib, and ATMSeer in its functionality. A comparison of the functionality with these software packages can be seen in Table 1. The features being compared are as follows: the major platform or language; whether the software has multiple users or multitenancy; can run AutoML libraries; is fault-tolerant; can store metrics in some database; can collect metrics; is scalable; is configurable; has visualization for different metrics; users can upload data or models; and has the capability to work with deep learning models. Both ATMSeer and Katib hold most of the functionality that THURSDAY provides. Katib is able to work with many different machine learning libraries and could use an AutoML library like AutoKeras, but it would be counterproductive as Katib itself is a system built to do AutoML with existing machine learning libraries. Katib is very similar to ATM and AutoKeras. Also, Katib is not built to be a visualization tool to compare machine learning libraries. ATMSeer, being a visualization tool, allows for the user to create custom models and is built on top of the ATM AutoML system. ATMSeer, however, does not provide the functionality to compare machine learning libraries against themselves or to run them. ATM was designed as a system that works separately from other AutoML libraries and does not seem to perform generation from deep learning based networks.

6. Conclusion
In this paper, we discussed how THURSDAY is implemented and what other tools or systems it is related to. THURSDAY is implemented as a full stack application that has a back end and a front end. The back end is comprised of a database and some web services that can send information from the database and start the generation of machine learning models with AutoML libraries. The front end allows for the user to interface with the back end to do things such as configuring the generation of machine models from an AutoML library, or visualizing results from a previous model with one dataset.

THURSDAY is a unique and modernized example of software engineering due to the inclusion of containerized elements with Docker, orchestrated containers with Docker-Compose, and the inclusion of Flask and Angular for abstracted user functions and front end design. THURSDAY is also very modular due to this containerization, so implementing other AutoML libraries and other datasets will be considerably easier than most other applications that aren’t containerized. THURSDAY also has the possibility to scale and be used by many users due to the containerization and orchestration done by Docker and Docker-Compose.